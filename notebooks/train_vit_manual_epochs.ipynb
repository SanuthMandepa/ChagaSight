{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612a80f4",
   "metadata": {},
   "source": [
    "### ChagaSight ‚Äî Vision Transformer (Baseline Training)\n",
    "\n",
    "Baseline ViT training on 2D ECG contour images  \n",
    "Datasets: PTB-XL (negatives), SaMi-Trop (positives), CODE-15 (soft labels)\n",
    "\n",
    "Baseline configuration:\n",
    "- 1% subset (pipeline verification)\n",
    "- No data augmentation\n",
    "- AMP enabled\n",
    "- Strict data integrity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f368854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "VRAM (GB): 6.441926656\n",
      "PROJECT_ROOT: d:\\IIT\\L6\\FYP\\ChagaSight\n",
      "DATA_DIR: d:\\IIT\\L6\\FYP\\ChagaSight\\data\\processed\n",
      "Experiment directory: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_baseline_1pct\\20251226_175406\n",
      "‚è± Cell 1 time: 0.04s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 1 (Code) ‚Äî Setup, device, paths, seed\n",
    "# =========================\n",
    "\n",
    "import time, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility (baseline-safe)\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Deterministic baseline (slower but reproducible)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", torch.cuda.get_device_properties(0).total_memory / 1e9)\n",
    "\n",
    "# -------------------------\n",
    "# Project root detection (VS Code safe)\n",
    "# -------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# -------------------------\n",
    "# Experiment folder (one folder per run)\n",
    "# -------------------------\n",
    "EXP_NAME = \"vit_baseline_1pct\"\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = PROJECT_ROOT / \"experiments\" / EXP_NAME / RUN_ID\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Experiment directory:\", EXP_DIR)\n",
    "\n",
    "print(f\"‚è± Cell 1 time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa73d6",
   "metadata": {},
   "source": [
    "#### Cell 1 ‚Äî What this does\n",
    "- Fixes randomness using a seed for reproducibility.\n",
    "- Detects GPU and prints VRAM.\n",
    "- Finds the project root robustly (works in VS Code even if the notebook is in `/notebooks`).\n",
    "- Creates a unique experiment run folder under `experiments/<EXP_NAME>/<RUN_ID>/`.\n",
    "\n",
    "#### Future improvements\n",
    "- Run multiple seeds (e.g., 5 runs) and report mean ¬± std AUROC.\n",
    "- Log CUDA + PyTorch versions for stronger reproducibility.\n",
    "- For speed-focused runs (not baseline), enable `torch.backends.cudnn.benchmark = True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71329e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total metadata rows (raw): 63228\n",
      "Rows after integrity filter: 63228\n",
      "Subset records (10%): 6323\n",
      "Train: 5058 | Val: 632 | Test: 633\n",
      "‚è± Cell 2 time: 4.14s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2 ‚Äî Metadata loading + integrity filtering + subset + splits\n",
    "# =========================\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Datasets included\n",
    "# -------------------------\n",
    "datasets = [\"ptbxl\", \"sami_trop\", \"code15\"]\n",
    "dfs = []\n",
    "\n",
    "for ds in datasets:\n",
    "    csv_path = DATA_DIR / \"metadata\" / f\"{ds}_metadata.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing metadata CSV: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"dataset\"] = ds\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all datasets\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Total metadata rows (raw):\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# HARD integrity filter (relative-path safe)\n",
    "# -------------------------\n",
    "def img_exists(p):\n",
    "    return (PROJECT_ROOT / Path(p)).exists()\n",
    "\n",
    "exists_mask = df_all[\"img_path\"].apply(img_exists)\n",
    "missing_count = (~exists_mask).sum()\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Dropping {missing_count} rows with missing image files\")\n",
    "    print(df_all.loc[~exists_mask, [\"dataset\", \"img_path\"]].head())\n",
    "\n",
    "df_all = df_all.loc[exists_mask].reset_index(drop=True)\n",
    "print(\"Rows after integrity filter:\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# Subset control\n",
    "# -------------------------\n",
    "# IMPORTANT:\n",
    "#   - Use 0.01 for smoke tests\n",
    "#   - Use 1.0 for full training\n",
    "subset_frac = 1.0\n",
    "\n",
    "if subset_frac < 1.0:\n",
    "    df_all = df_all.sample(frac=subset_frac, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Subset records ({subset_frac*100:.0f}%):\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# Binary label ONLY for stratification / metrics\n",
    "# (Model still uses soft labels)\n",
    "# -------------------------\n",
    "df_all[\"label_bin\"] = (df_all[\"label\"] > 0.5).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Val / Test split (80 / 10 / 10)\n",
    "# -------------------------\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.2,\n",
    "    stratify=df_all[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"‚è± Cell 2 time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc685d",
   "metadata": {},
   "source": [
    "#### Cell 2 ‚Äî What this does\n",
    "- Loads metadata CSVs for PTB-XL, SaMi-Trop, and CODE-15.\n",
    "- Drops any rows where the `.npy` image file is missing (prevents DataLoader crashes).\n",
    "- Samples a 1% subset for fast validation that the pipeline is correct.\n",
    "- Creates stratified Train/Val/Test splits using a binary label for metrics only.\n",
    "\n",
    "#### Future improvements\n",
    "- Scale from `subset_frac=0.01` ‚Üí `0.10` ‚Üí `1.0` once stable.\n",
    "- Consider dataset-aware splits (e.g., holding out one dataset for domain generalisation testing).\n",
    "- If patient IDs exist, enforce patient-wise splitting to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a741ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full-dataset training (natural class distribution)\n",
      "‚úì Batch image shape : torch.Size([16, 3, 24, 2048])\n",
      "‚úì Sample labels    : [0.0, 0.20000000298023224, 0.0, 0.0, 0.20000000298023224, 0.20000000298023224, 0.0, 0.0, 0.20000000298023224, 0.20000000298023224]\n",
      "‚úì Image range      : [0.0, 255.0]\n",
      "Train samples : 5058\n",
      "Train batches : 317\n",
      "‚è± Cell 3 time: 0.26s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 ‚Äî Dataset + DataLoaders (FINAL, research-correct)\n",
    "# =========================\n",
    "import time\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class ECGImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for 2D ECG image embeddings.\n",
    "\n",
    "    Labels are SOFT labels used for weak supervision:\n",
    "    - PTB-XL: 0.0 (definite negative)\n",
    "    - SaMi-Trop: 1.0 (definite positive)\n",
    "    - CODE-15: soft uncertainty labels (e.g., 0.2 / 0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = PROJECT_ROOT / Path(row[\"img_path\"])\n",
    "\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image file: {img_path}\")\n",
    "\n",
    "        img = np.load(img_path).astype(np.float32)\n",
    "\n",
    "        # Strict research safety\n",
    "        if img.shape != (3, 24, 2048):\n",
    "            raise ValueError(f\"Invalid image shape {img.shape} at {img_path}\")\n",
    "\n",
    "        # üî¥ CRITICAL FIX: normalize for ViT stability\n",
    "        img = img / 255.0  # scale to [0,1]\n",
    "\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        # Soft label preserved\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "batch_size = 16  # RTX 3050 6GB safe\n",
    "\n",
    "train_ds = ECGImageDataset(train_df)\n",
    "val_ds   = ECGImageDataset(val_df)\n",
    "test_ds  = ECGImageDataset(test_df)\n",
    "\n",
    "# Sampler logic\n",
    "if subset_frac < 0.1:\n",
    "    print(\"‚ö†Ô∏è Oversampling enabled (debug subset)\")\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "    weights = train_df[\"label\"].apply(lambda x: 10.0 if x > 0.7 else 1.0).values\n",
    "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ Full dataset training (natural distribution)\")\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(\"‚úì Batch shape:\", x_batch.shape)\n",
    "print(\"‚úì Image range:\", x_batch.min().item(), x_batch.max().item())\n",
    "print(\"‚úì Sample labels:\", y_batch[:10].tolist())\n",
    "print(f\"‚è± Cell 3 time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b1f09",
   "metadata": {},
   "source": [
    "### Why image normalisation is required\n",
    "Vision Transformers are sensitive to input scale because attention scores\n",
    "are computed directly from dot products. Normalising ECG images to [0,1]\n",
    "ensures stable optimisation and fair comparison across datasets.\n",
    "\n",
    "### Why soft labels are used\n",
    "CODE-15 annotations reflect uncertainty rather than binary truth.\n",
    "Soft-label training enables weak supervision and avoids forcing noisy\n",
    "labels into hard categories, aligning with the referenced ECG foundation\n",
    "model literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84669f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT trainable parameters: 85,747,201\n",
      "‚úì Forward OK | logits shape: torch.Size([16])\n",
      "‚úì Peak GPU memory used (GB): 0.44\n",
      "‚è± Cell 4 time: 1.65s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4 ‚Äî ViT model + forward sanity + peak memory (clean)\n",
    "# =========================\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_ch=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (24 // patch_size) * (2048 // patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                  # (B, E, H', W')\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, N, E)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        x = x + y\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, patch_size=16, embed_dim=768, depth=12, heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size=patch_size, in_ch=3, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim=embed_dim, heads=heads, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "# Instantiate\n",
    "model = ViTClassifier().to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"ViT trainable parameters: {num_params:,}\")\n",
    "\n",
    "# Forward sanity\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x_batch.to(device))\n",
    "\n",
    "print(\"‚úì Forward OK | logits shape:\", logits.shape)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"‚úì Peak GPU memory used (GB): {peak_mem:.2f}\")\n",
    "\n",
    "print(f\"‚è± Cell 4 time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6d907",
   "metadata": {},
   "source": [
    "#### Cell 4 ‚Äî What this does\n",
    "- Defines a ViT-B/16-like classifier from scratch.\n",
    "- Confirms that the model forward pass works on a real batch.\n",
    "- Prints peak GPU memory usage for sanity.\n",
    "\n",
    "#### Future improvements\n",
    "- Try smaller models for ablation (e.g., depth=8, embed_dim=512).\n",
    "- Add regularisation tuning (dropout, stochastic depth) for full-scale training.\n",
    "- Consider pretraining (foundation model) before supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d62cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "AMP enabled: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b30d9600eba4018a632c440db1fb114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 [Train]:   0%|          | 0/317 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570982dec7834a928e574d7eddff9cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 [Val]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss(avg)=0.4588 | val AUROC=0.6563 ‚úÖ | time=147.2s\n",
      "\n",
      "Training complete.\n",
      "Best validation AUROC: 0.6563146997929605\n",
      "Saved best model to: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_baseline_1pct\\20251226_175406\\model_best.pth\n",
      "Saved metrics to: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_baseline_1pct\\20251226_175406\\metrics.csv\n",
      "Saved config to: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_baseline_1pct\\20251226_175406\\config.json\n",
      "‚è± Cell 5 total time: 149.29s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5 ‚Äî Training loop FIXED (AMP warnings removed + avg loss + full logging)\n",
    "# =========================\n",
    "import time, json\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Training configuration\n",
    "# -------------------------\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.05\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "\n",
    "# NEW AMP API (removes FutureWarning)\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "best_val_auc = 0.0\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"AMP enabled:\", use_amp)\n",
    "\n",
    "# Save full config\n",
    "config = {\n",
    "    \"experiment_name\": EXP_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"seed\": SEED,\n",
    "    \"datasets\": [\"ptbxl\", \"sami_trop\", \"code15\"],\n",
    "    \"subset_frac\": subset_frac,\n",
    "    \"train/val/test_sizes\": {\"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df)},\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"amp\": use_amp,\n",
    "    \"input_shape\": [3, 24, 2048],\n",
    "    \"vit\": {\"patch_size\": 16, \"embed_dim\": 768, \"depth\": 12, \"heads\": 12, \"mlp_ratio\": 4.0, \"dropout\": 0.1},\n",
    "}\n",
    "if device.type == \"cuda\":\n",
    "    config[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "    config[\"vram_gb\"] = float(torch.cuda.get_device_properties(0).total_memory / 1e9)\n",
    "\n",
    "with open(EXP_DIR / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# Epoch loop\n",
    "# -------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "\n",
    "    for imgs, labels in train_bar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    train_loss = running_loss / max(1, n_batches)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval()\n",
    "    val_preds, val_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            probs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
    "            val_preds.extend(probs)\n",
    "            val_trues.extend(labels.numpy())\n",
    "\n",
    "    val_trues = np.asarray(val_trues)\n",
    "    val_preds = np.asarray(val_preds)\n",
    "\n",
    "    # Binary labels for metric ONLY\n",
    "    val_trues_bin = (val_trues > 0.5).astype(int)\n",
    "    val_auc = roc_auc_score(val_trues_bin, val_preds)\n",
    "\n",
    "    # Save best model\n",
    "    improved = \"\"\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        improved = \"‚úÖ\"\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_avg\": float(train_loss),\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"epoch_time_sec\": float(epoch_time),\n",
    "        \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"loss(avg)={train_loss:.4f} | \"\n",
    "        f\"val AUROC={val_auc:.4f} {improved} | \"\n",
    "        f\"time={epoch_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "# Save metrics.csv\n",
    "pd.DataFrame(history).to_csv(EXP_DIR / \"metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"Best validation AUROC:\", best_val_auc)\n",
    "print(\"Saved best model to:\", best_model_path)\n",
    "print(\"Saved metrics to:\", EXP_DIR / \"metrics.csv\")\n",
    "print(\"Saved config to:\", EXP_DIR / \"config.json\")\n",
    "print(f\"‚è± Cell 5 total time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e2eee",
   "metadata": {},
   "source": [
    "#### Cell 5 ‚Äî What this does\n",
    "- Trains the ViT model for a fixed number of epochs.\n",
    "- Uses AMP (mixed precision) to reduce VRAM usage and improve speed on the RTX 3050.\n",
    "- Computes validation AUROC each epoch (binary threshold only for the metric, not for training).\n",
    "- Saves a fully reproducible experiment record:\n",
    "  - `config.json` (model + hyperparameters + data sizes + GPU info)\n",
    "  - `metrics.csv` (epoch-by-epoch loss, AUROC, timing, LR)\n",
    "  - `model_best.pth` (best checkpoint selected by validation AUROC)\n",
    "\n",
    "#### Future improvements\n",
    "- Increase epochs for full-scale training (10‚Äì50).\n",
    "- Add early stopping based on AUROC plateau.\n",
    "- Replace oversampling with class-weighted loss or focal loss (ablation).\n",
    "- Add AUPRC and threshold metrics for clinical relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5398e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_baseline_1pct\\20251226_175406\\model_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc559fedddc44afa4d5f5cdfa9a87f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test evaluation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST RESULTS ===\n",
      "Test AUROC : 0.6843\n",
      "‚è± Cell 6 time: 11.66s\n",
      "Saved test results to: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_baseline_1pct\\20251226_175406\\test_results.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6 ‚Äî Test evaluation + save test_results.json (full)\n",
    "# =========================\n",
    "import time, json\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load best model from this run folder\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "assert best_model_path.exists(), \"Best model checkpoint not found!\"\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded best model from:\", best_model_path)\n",
    "\n",
    "test_preds, test_trues = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Test evaluation\"):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        probs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
    "        test_preds.extend(probs)\n",
    "        test_trues.extend(labels.numpy())\n",
    "\n",
    "test_preds = np.asarray(test_preds)\n",
    "test_trues = np.asarray(test_trues)\n",
    "\n",
    "# Binary labels ONLY for metric\n",
    "test_trues_bin = (test_trues > 0.5).astype(int)\n",
    "test_auc = roc_auc_score(test_trues_bin, test_preds)\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "print(f\"Test AUROC : {test_auc:.4f}\")\n",
    "print(f\"‚è± Cell 6 time: {test_time:.2f}s\")\n",
    "\n",
    "# Save test results\n",
    "test_results = {\n",
    "    \"test_auc\": float(test_auc),\n",
    "    \"num_test_samples\": int(len(test_trues)),\n",
    "    \"evaluation_time_sec\": float(test_time),\n",
    "}\n",
    "with open(EXP_DIR / \"test_results.json\", \"w\") as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"Saved test results to:\", EXP_DIR / \"test_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a72223",
   "metadata": {},
   "source": [
    "##### Cell 6 ‚Äî What this does\n",
    "- Loads the best checkpoint from this run folder.\n",
    "- Evaluates on the held-out test split.\n",
    "- Saves `test_results.json` so results are permanently stored.\n",
    "\n",
    "##### Future improvements\n",
    "- Report confidence intervals via bootstrapping (important for medical AI).\n",
    "- Add subgroup evaluation by dataset (PTB-XL vs SaMi-Trop vs CODE-15).\n",
    "- Add threshold-based metrics (sensitivity, specificity) for clinical interpretation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
