{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612a80f4",
   "metadata": {},
   "source": [
    "### ChagaSight ‚Äî Vision Transformer (Baseline Training)\n",
    "\n",
    "Baseline ViT training on 2D ECG contour images  \n",
    "Datasets: PTB-XL (negatives), SaMi-Trop (positives), CODE-15 (soft labels)\n",
    "\n",
    "Baseline configuration:\n",
    "- 1% subset (pipeline verification)\n",
    "- No data augmentation\n",
    "- AMP enabled\n",
    "- Strict data integrity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f368854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "VRAM (GB): 6.441926656\n",
      "PROJECT_ROOT: d:\\IIT\\L6\\FYP\\ChagaSight\n",
      "DATA_DIR: d:\\IIT\\L6\\FYP\\ChagaSight\\data\\processed\n",
      "Experiment directory: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_contour_baseline\\20251227_095949\n",
      "‚è± Cell 1 time: 0.02s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 1 (Code) ‚Äî Setup, device, paths, seed + GPU Monitoring\n",
    "# =========================\n",
    "\n",
    "import time, random, sys\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility (Baseline-Safe)\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Deterministic baseline (Crucial for medical AI research)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Device Configuration\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", torch.cuda.get_device_properties(0).total_memory / 1e9)\n",
    "\n",
    "# -------------------------\n",
    "# Optional: Live GPU Monitoring (nvidia-smi in background)\n",
    "# -------------------------\n",
    "def monitor_gpu():\n",
    "    \"\"\"Runs nvidia-smi every 5 seconds in a background thread for live monitoring.\"\"\"\n",
    "    try:\n",
    "        subprocess.Popen([\n",
    "            'nvidia-smi',\n",
    "            '--query-gpu=timestamp,name,utilization.gpu,memory.used,memory.total',\n",
    "            '--format=csv',\n",
    "            '-l', '5'  # Update every 5 seconds\n",
    "        ])\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: nvidia-smi not found ‚Äì GPU monitoring skipped (normal on some systems).\")\n",
    "\n",
    "# Start monitoring in a daemon thread (won't block notebook shutdown)\n",
    "threading.Thread(target=monitor_gpu, daemon=True).start()\n",
    "\n",
    "# -------------------------\n",
    "# Universal Project Root Detection\n",
    "# -------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"Finds the directory containing the 'data' folder.\"\"\"\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "# Set current working directory based on the notebook location\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Add project root to sys.path for importing local src modules\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Experiment Management\n",
    "# -------------------------\n",
    "# Final year project name aligned with research papers\n",
    "EXP_NAME = \"vit_contour_baseline\" \n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = PROJECT_ROOT / \"experiments\" / EXP_NAME / RUN_ID\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Experiment directory:\", EXP_DIR)\n",
    "\n",
    "print(f\"‚è± Cell 1 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa73d6",
   "metadata": {},
   "source": [
    "#### Cell 1 ‚Äî What this does\n",
    "- Fixes randomness using a fixed seed (SEED=42) across Python, NumPy, and PyTorch for full reproducibility.\n",
    "- Enforces deterministic behavior (`torch.backends.cudnn.deterministic = True`) ‚Äî critical for medical AI research.\n",
    "- Detects GPU, prints device name and total VRAM.\n",
    "- **Launches live GPU monitoring** (`nvidia-smi` every 5 seconds in background) to track utilization % and memory usage during training.\n",
    "- Finds the project root robustly by locating the `data` folder ‚Äî works reliably even if the notebook is opened from a subdirectory (e.g., `/notebooks` in VS Code).\n",
    "- Creates a unique, timestamped experiment folder under `experiments/vit_contour_baseline/<RUN_ID>/` for saving logs, models, and metrics.\n",
    "\n",
    "#### Future improvements\n",
    "- Run multiple seeds (e.g., 5 independent runs with SEED=42, 123, 456, ...) and report mean ¬± std for AUROC, AUPRC, F1, and Challenge score.\n",
    "- Log PyTorch, CUDA, cuDNN, and Transformers versions automatically for stronger reproducibility across machines.\n",
    "- For non-baseline (speed-focused) experiments, enable `torch.backends.cudnn.benchmark = True` to gain extra performance on fixed input sizes.\n",
    "- Add Weights & Biases (wandb) or TensorBoard integration for richer experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71329e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total metadata rows (raw): 63228\n",
      "Checking existence of image files...\n",
      "Rows after integrity filter: 63228\n",
      "Final records used (1% of clean data): 632\n",
      "Train: 505 | Val: 63 | Test: 64\n",
      "Train class distribution: 0.1469 (mean probability)\n",
      "Val   class distribution: 0.1397\n",
      "Test  class distribution: 0.1625\n",
      "‚è± Cell 2 time: 3.37s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 2 (Code) ‚Äî Metadata loading + integrity filtering + subset + splits (Optimized)\n",
    "# =========================\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Datasets included (Kim et al. & Van Santvliet et al. benchmarks)\n",
    "# -------------------------\n",
    "datasets = [\"ptbxl\", \"sami_trop\", \"code15\"]\n",
    "dfs = []\n",
    "\n",
    "for ds in datasets:\n",
    "    csv_path = DATA_DIR / \"metadata\" / f\"{ds}_metadata.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing metadata CSV: {csv_path}\")\n",
    "\n",
    "    # Use low_memory=False to avoid DtypeWarning on large CSVs\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    df[\"dataset\"] = ds\n",
    "    \n",
    "    # Ensure labels are floats for soft-label support (0.2 / 0.8)\n",
    "    df[\"label\"] = df[\"label\"].astype(float)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all datasets into a unified research corpus\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Total metadata rows (raw):\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# HARD Integrity Filter (Cross-platform path safe + Faster)\n",
    "# -------------------------\n",
    "def img_exists(p):\n",
    "    # Convert string path to Path object once, then resolve relative to PROJECT_ROOT\n",
    "    return (PROJECT_ROOT / Path(str(p))).exists()\n",
    "\n",
    "print(\"Checking existence of image files...\")\n",
    "exists_mask = df_all[\"img_path\"].apply(img_exists)\n",
    "missing_count = (~exists_mask).sum()\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Dropping {missing_count} rows with missing image files\")\n",
    "    # Show only first 5 for brevity\n",
    "    print(df_all.loc[~exists_mask, [\"dataset\", \"img_path\"]].head())\n",
    "\n",
    "df_all = df_all.loc[exists_mask].reset_index(drop=True)\n",
    "print(\"Rows after integrity filter:\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# Subset Control (Final runs use full dataset)\n",
    "# -------------------------\n",
    "# Research Note: Use 1.0 for final results. Lower values only for quick debugging.\n",
    "subset_frac = 0.01  # ‚Üê Kept as 1.0 for full dataset training\n",
    "\n",
    "if subset_frac < 1.0:\n",
    "    df_all = df_all.sample(frac=subset_frac, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final records used ({subset_frac*100:.0f}% of clean data):\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# Binary Stratification (Clinical Ground Truth)\n",
    "# -------------------------\n",
    "# Binary label used only for stratified splitting (preserves class balance)\n",
    "df_all[\"label_bin\"] = (df_all[\"label\"] > 0.5).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Val / Test split (80 / 10 / 10) with stratification\n",
    "# -------------------------\n",
    "# Stratification ensures consistent Chagas prevalence across splits\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.2,\n",
    "    stratify=df_all[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Drop temporary binary column to keep DataFrames clean\n",
    "train_df = train_df.drop(columns=[\"label_bin\"])\n",
    "val_df   = val_df.drop(columns=[\"label_bin\"])\n",
    "test_df  = test_df.drop(columns=[\"label_bin\"])\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"Train class distribution: {train_df['label'].mean():.4f} (mean probability)\")\n",
    "print(f\"Val   class distribution: {val_df['label'].mean():.4f}\")\n",
    "print(f\"Test  class distribution: {test_df['label'].mean():.4f}\")\n",
    "\n",
    "print(f\"‚è± Cell 2 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc685d",
   "metadata": {},
   "source": [
    "#### Cell 2 ‚Äî What this does\n",
    "- Loads metadata CSVs for PTB-XL, SaMi-Trop, and CODE-15% with efficient parsing.\n",
    "- Strictly filters out any records where the corresponding 2D contour image (`.npy`) is missing ‚Äî prevents DataLoader crashes.\n",
    "- Uses the **full clean dataset** (`subset_frac=1.0`) for final training (as required for benchmark comparison with Kim et al. and Van Santvliet et al.).\n",
    "- Creates stratified Train (80%) / Val (10%) / Test (10%) splits using binarized labels (>0.5) to preserve Chagas prevalence across sets.\n",
    "- Prints class balance to verify successful stratification.\n",
    "\n",
    "#### Future improvements\n",
    "- Implement **dataset-wise held-out testing** (e.g., train on PTB-XL + CODE-15%, test on SaMi-Trop) for stronger domain generalization evaluation.\n",
    "- If patient-level IDs become available, switch to **patient-wise splitting** to prevent data leakage.\n",
    "- Add Weights & Biases logging of split statistics and dataset distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a741ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full dataset training mode enabled (natural class distribution)\n",
      "   Train batches per epoch: 32\n",
      "   Val   batches per epoch: 4\n",
      "   Test  batches per epoch: 4\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 ‚Äî Dataset + DataLoaders (HIGH-SPEED, research-correct, RTX 3050 optimized)\n",
    "# =========================\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class ECGImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Optimized Dataset for 2D ECG contour images (Kim et al. 2025).\n",
    "    \n",
    "    Key research decisions preserved:\n",
    "    - Soft labels for weak supervision (PTB-XL: 0.0, SaMi-Trop: 1.0, CODE-15%: 0.2/0.8)\n",
    "    - Zero-centered normalization to [-1, 1] for stable ViT attention\n",
    "    - Strict shape validation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        # Pre-compute resolved paths for faster __getitem__\n",
    "        self.img_paths = [(PROJECT_ROOT / Path(str(p))).resolve() for p in self.df[\"img_path\"]]\n",
    "        self.labels = self.df[\"label\"].astype(np.float32).values  # Convert once to numpy array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "\n",
    "        # Fast .npy loading with memory-mapping (safe for full dataset)\n",
    "        img = np.load(img_path, mmap_mode='r').astype(np.float32)\n",
    "\n",
    "        # Strict shape check (kept for research integrity)\n",
    "        if img.shape != (3, 24, 2048):\n",
    "            raise ValueError(f\"Invalid image shape {img.shape} at {img_path}\")\n",
    "\n",
    "        # Zero-centered normalization: [0, 255] ‚Üí [-1, 1] (critical for ViT stability)\n",
    "        img = (img / 127.5) - 1.0\n",
    "\n",
    "        img = torch.from_numpy(img)  # Creates contiguous tensor\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# -------------------------\n",
    "# Batch Size & DataLoader Configuration (RTX 3050 6GB Optimized)\n",
    "# -------------------------\n",
    "batch_size = 16  # Safe and efficient for ViT-small/base with rectangular patches on 6GB VRAM\n",
    "\n",
    "train_ds = ECGImageDataset(train_df)\n",
    "val_ds   = ECGImageDataset(val_df)\n",
    "test_ds  = ECGImageDataset(test_df)\n",
    "\n",
    "# High-performance DataLoader settings\n",
    "dataloader_kwargs = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"pin_memory\": True,           # Fast CPU ‚Üí GPU transfer\n",
    "    \"prefetch_factor\": 4,         # Preload 4 batches ahead per worker\n",
    "    \"persistent_workers\": True,   # Workers stay alive between epochs ‚Üí huge speedup after epoch 1\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    shuffle=True,                 # Natural shuffling for full dataset\n",
    "    num_workers=6,                # ‚Üê MAJOR SPEEDUP: Use 6 workers (your 12th-gen CPU can handle it)\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    shuffle=False,\n",
    "    num_workers=6,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    shuffle=False,\n",
    "    num_workers=6,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Full dataset training mode enabled (natural class distribution)\")\n",
    "print(f\"   Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"   Val   batches per epoch: {len(val_loader)}\")\n",
    "print(f\"   Test  batches per epoch: {len(test_loader)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Sanity Check\n",
    "# -------------------------\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"‚úì Batch shape: {x_batch.shape}\")          # Expected: [16, 3, 24, 2048]\n",
    "print(f\"‚úì Image range: {x_batch.min().item():.3f} to {x_batch.max().item():.3f}\")  # Should be ~ -1.0 to 1.0\n",
    "print(f\"‚úì Sample labels: {y_batch[:8].tolist()}\")\n",
    "\n",
    "print(f\"‚è± Cell 3 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b1f09",
   "metadata": {},
   "source": [
    "### Cell 3 ‚Äî What this does\n",
    "- Implements a **high-performance** `ECGIImageDataset` with:\n",
    "  - Pre-cached absolute image paths and labels\n",
    "  - Memory-mapped `.npy` loading (`mmap_mode='r'`) ‚Üí safe and fast on full dataset\n",
    "  - Zero-centered normalization to [-1, 1] (essential for stable ViT training)\n",
    "  - Strict shape validation for research integrity\n",
    "- Creates **optimized DataLoaders** using:\n",
    "  - `num_workers=6`, `persistent_workers=True`, `prefetch_factor=4` ‚Üí eliminates CPU bottleneck\n",
    "  - `pin_memory=True` for fastest transfer to GPU\n",
    "- Trains on **full dataset** with natural class distribution (no artificial oversampling)\n",
    "\n",
    "### Why this normalization is required\n",
    "Vision Transformers rely on dot-product attention. Mapping ECG values to [-1, 1] (zero-centered) prevents gradient instability and ensures fair feature learning across heterogeneous datasets ‚Äî a key factor in Kim et al. and Van Santvliet et al.\n",
    "\n",
    "### Why soft labels are used\n",
    "CODE-15% labels contain self-reported diagnostic noise. Using soft targets (0.2/0.8) with BCEWithLogitsLoss enables robust weak supervision, improving generalization ‚Äî standard practice in modern ECG deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84669f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 4 ‚Äî ViT model (ViT-Small, RTX 3050 optimized) + forward sanity + peak memory\n",
    "# =========================\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_h=8, patch_w=16, in_ch=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        # Rectangular patches perfectly tile 24√ó2048 ‚Üí 3√ó128 = 384 patches\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))\n",
    "        self.num_patches = (24 // patch_h) * (2048 // patch_w)  # = 384\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                  # (B, embed_dim, 3, 128)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, 384, embed_dim)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=384, heads=6, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        x = x + y\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT-Small/8√ó16 ‚Äî ~22M parameters (vs ~86M in ViT-Base)\n",
    "    Perfectly matches Kim et al. rectangular patch strategy while being 4√ó lighter and faster.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_h=8, patch_w=16, embed_dim=384, depth=12, heads=6, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_h=patch_h, patch_w=patch_w, in_ch=3, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches  # 384\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim=embed_dim, heads=heads, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, 1)  # Single logit for BCEWithLogitsLoss\n",
    "\n",
    "        # Standard initialization for stable training\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)      # (B, 385, embed_dim)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])                     # CLS token only\n",
    "        return self.head(x).squeeze(-1)            # (B,)\n",
    "\n",
    "# Instantiate ViT-Small with rectangular patches\n",
    "model = ViTClassifier(\n",
    "    patch_h=8,\n",
    "    patch_w=16,\n",
    "    embed_dim=384,    # ‚Üê ViT-Small dimension (instead of 768)\n",
    "    depth=12,\n",
    "    heads=6           # 384 / 64 = 6 heads\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"ViT-Small trainable parameters: {num_params:,}\")  # ~22M\n",
    "\n",
    "# Forward sanity check on real batch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x_batch.to(device))\n",
    "\n",
    "print(f\"‚úì Forward pass successful | Logits shape: {logits.shape}\")  # [B]\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "    current_mem = torch.cuda.memory_allocated(device) / 1e9\n",
    "    print(f\"‚úì Peak GPU memory used: {peak_mem:.2f} GB\")\n",
    "    print(f\"‚úì Current GPU memory: {current_mem:.2f} GB\")\n",
    "\n",
    "print(f\"‚è± Cell 4 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6d907",
   "metadata": {},
   "source": [
    "#### Cell 4 ‚Äî What this does\n",
    "- Implements **ViT-Small/8√ó16** (~22M parameters) with rectangular patches ‚Äî perfectly tiles 24√ó2048 contour images (384 patches + CLS token).\n",
    "- Uses standard ViT initialization for stable training from scratch.\n",
    "- Verifies forward pass on a real batch and reports peak/current GPU memory.\n",
    "- Designed for efficient training on RTX 3050 6GB while preserving full spatial information (Kim et al. 2025).\n",
    "\n",
    "#### Future improvements\n",
    "- Add stochastic depth (deep norm) for even better training stability.\n",
    "- Experiment with pretraining on large unlabeled ECGs (foundation model style ‚Äî Van Santvliet et al.).\n",
    "- Try hybrid alignment with 1D ECG-FM features via cosine similarity (REPA-style, Kim et al.).\n",
    "- Use mixed-precision training (AMP) with gradient scaling in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 5 ‚Äî Training loop (FULLY OPTIMIZED: AMP + Gradient Accumulation + Full Metrics + RTX 3050 safe)\n",
    "# =========================\n",
    "import time, json\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "# -------------------------\n",
    "# Training configuration (RTX 3050 optimized)\n",
    "# -------------------------\n",
    "num_epochs = 20\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Effective larger batch via accumulation (stabilizes training with ViT-Small)\n",
    "accumulation_steps = 2          # Effective batch_size = 16 √ó 2 = 32\n",
    "effective_batch_size = batch_size * accumulation_steps\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Perfect for soft labels\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "best_val_auc = 0.0\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "history = []\n",
    "\n",
    "print(\"Starting training on full dataset...\")\n",
    "print(f\"AMP enabled: {use_amp} | Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "# -------------------------\n",
    "# Updated Config Logging (reflects ViT-Small + optimizations)\n",
    "# -------------------------\n",
    "config = {\n",
    "    \"experiment_name\": EXP_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"seed\": SEED,\n",
    "    \"datasets\": [\"ptbxl\", \"sami_trop\", \"code15\"],\n",
    "    \"subset_frac\": subset_frac,\n",
    "    \"train/val/test_sizes\": {\"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df)},\n",
    "    \"batch_size\": batch_size,\n",
    "    \"accumulation_steps\": accumulation_steps,\n",
    "    \"effective_batch_size\": effective_batch_size,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"amp\": use_amp,\n",
    "    \"input_shape\": [3, 24, 2048],\n",
    "    \"vit\": {\n",
    "        \"variant\": \"ViT-Small\",\n",
    "        \"patch_h\": 8,\n",
    "        \"patch_w\": 16,\n",
    "        \"embed_dim\": 384,\n",
    "        \"depth\": 12,\n",
    "        \"heads\": 6,\n",
    "        \"mlp_ratio\": 4.0,\n",
    "        \"dropout\": 0.1,\n",
    "        \"num_parameters\": num_params\n",
    "    },\n",
    "}\n",
    "if device.type == \"cuda\":\n",
    "    config[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "    config[\"vram_gb\"] = round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2)\n",
    "\n",
    "with open(EXP_DIR / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# Epoch loop with gradient accumulation + full metrics\n",
    "# -------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1:02d}/{num_epochs} [Train]\", leave=False)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (imgs, labels) in enumerate(train_bar):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss = loss / accumulation_steps  # Normalize for accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps  # Undo normalization for reporting\n",
    "        n_batches += 1\n",
    "\n",
    "        train_bar.set_postfix({\"loss\": f\"{running_loss/n_batches:.4f}\"})\n",
    "\n",
    "    train_loss = running_loss / max(1, n_batches)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_preds, val_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1:02d}/{num_epochs} [Val]\", leave=False):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            logits = model(imgs)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            val_preds.extend(probs)\n",
    "            val_trues.extend(labels.numpy())\n",
    "\n",
    "    val_trues = np.array(val_trues)\n",
    "    val_preds = np.array(val_preds)\n",
    "\n",
    "    # Full metrics (aligned with papers + challenge)\n",
    "    val_trues_bin = (val_trues > 0.5).astype(int)\n",
    "    val_preds_bin = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "    val_auc = roc_auc_score(val_trues_bin, val_preds)\n",
    "    val_auprc = average_precision_score(val_trues_bin, val_preds)\n",
    "    val_acc = accuracy_score(val_trues_bin, val_preds_bin)\n",
    "    val_f1 = f1_score(val_trues_bin, val_preds_bin)\n",
    "\n",
    "    # Challenge-specific prioritization score (fraction of positives in top 5%)\n",
    "    sorted_idx = np.argsort(val_preds)[::-1]\n",
    "    top_5_pct = int(0.05 * len(val_preds))\n",
    "    positives_in_top = val_trues_bin[sorted_idx[:top_5_pct]].sum()\n",
    "    challenge_score = positives_in_top / val_trues_bin.sum() if val_trues_bin.sum() > 0 else 0.0\n",
    "\n",
    "    # Save best model (primary metric: AUROC)\n",
    "    improved = \"\"\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_auc': val_auc,\n",
    "            'epoch': epoch + 1\n",
    "        }, best_model_path)\n",
    "        improved = \"‚úÖ\"\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": round(float(train_loss), 6),\n",
    "        \"val_auc\": round(float(val_auc), 6),\n",
    "        \"val_auprc\": round(float(val_auprc), 6),\n",
    "        \"val_acc\": round(float(val_acc), 6),\n",
    "        \"val_f1\": round(float(val_f1), 6),\n",
    "        \"challenge_score\": round(float(challenge_score), 6),\n",
    "        \"epoch_time_sec\": round(float(epoch_time), 1),\n",
    "        \"lr\": round(float(optimizer.param_groups[0][\"lr\"]), 8),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Loss: {train_loss:.4f} | \"\n",
    "        f\"AUROC: {val_auc:.4f} | AUPRC: {val_auprc:.4f} | \"\n",
    "        f\"F1: {val_f1:.4f} | Challenge: {challenge_score:.4f} {improved} | \"\n",
    "        f\"Time: {epoch_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Final save\n",
    "# -------------------------\n",
    "pd.DataFrame(history).to_csv(EXP_DIR / \"metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "print(f\"Best validation AUROC: {best_val_auc:.6f}\")\n",
    "print(f\"Saved best model: {best_model_path}\")\n",
    "print(f\"Full metrics: {EXP_DIR / 'metrics.csv'}\")\n",
    "print(f\"Config saved: {EXP_DIR / 'config.json'}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    final_peak = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "    print(f\"Final peak GPU memory: {final_peak:.2f} GB\")\n",
    "\n",
    "print(f\"‚è± Cell 5 total time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e2eee",
   "metadata": {},
   "source": [
    "#### Cell 5 ‚Äî What this does\n",
    "- Trains **ViT-Small** for 20 epochs with:\n",
    "  - Mixed precision (AMP) ‚Üí faster + lower VRAM\n",
    "  - Gradient accumulation (effective batch=32) ‚Üí more stable gradients\n",
    "  - Cosine LR scheduling\n",
    "- Evaluates **full benchmark metrics** each epoch:\n",
    "  - AUROC, AUPRC, Accuracy, F1\n",
    "  - **Challenge Score** (fraction of Chagas cases in top 5% ranked by probability)\n",
    "- Saves:\n",
    "  - `config.json` (complete hyperparams + model spec + hardware)\n",
    "  - `metrics.csv` (all epoch metrics)\n",
    "  - `model_best.pth` (best by validation AUROC)\n",
    "\n",
    "#### Future improvements\n",
    "- Add **early stopping** (e.g., patience=7 on AUROC plateau).\n",
    "- Implement **test-time evaluation** on held-out test set after training.\n",
    "- Add **learning rate warmup** for first 5 epochs.\n",
    "- Integrate **Weights & Biases** or **TensorBoard** for live plotting.\n",
    "- Try **focal loss** or **label smoothing** for better soft-label handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 6 ‚Äî Final Test Evaluation + Comprehensive Reporting (RESEARCH-READY)\n",
    "# =========================\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, accuracy_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Load Best Model Checkpoint\n",
    "# -------------------------\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "if not best_model_path.exists():\n",
    "    raise FileNotFoundError(f\"Best model not found at {best_model_path}\")\n",
    "\n",
    "# Safe loading (weights_only=True recommended in PyTorch ‚â•2.0)\n",
    "checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint if isinstance(checkpoint, dict) else checkpoint)  # Handles both dict and state_dict\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint.get('epoch', 'unknown')} with val_auc {checkpoint.get('val_auc', 'N/A'):.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Full Inference on Test Set\n",
    "# -------------------------\n",
    "test_preds = []\n",
    "test_trues = []\n",
    "test_ids = []           # For potential per-sample analysis\n",
    "test_datasets = []      # For subgroup analysis\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Final Test Inference\", leave=False):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "\n",
    "        test_preds.extend(probs.tolist() if probs.ndim > 0 else [float(probs)])\n",
    "        test_trues.extend(labels.numpy().tolist())\n",
    "        # Extract corresponding metadata for subgroup analysis\n",
    "        # Assuming test_loader dataset has access to df indices (we use batch size alignment)\n",
    "        # Simpler: use pre-stored test_df indices via enumerate tracking if needed\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "test_trues = np.array(test_trues)\n",
    "test_trues_bin = (test_trues > 0.5).astype(int)\n",
    "test_preds_bin = (test_preds >= 0.5).astype(int)\n",
    "\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "# -------------------------\n",
    "# Global Metrics (Full Benchmark Set)\n",
    "# -------------------------\n",
    "global_metrics = {\n",
    "    \"test_auc\": float(roc_auc_score(test_trues_bin, test_preds)),\n",
    "    \"test_auprc\": float(average_precision_score(test_trues_bin, test_preds)),\n",
    "    \"test_accuracy\": float(accuracy_score(test_trues_bin, test_preds_bin)),\n",
    "    \"test_f1\": float(f1_score(test_trues_bin, test_preds_bin)),\n",
    "    \"test_precision\": float(precision_score(test_trues_bin, test_preds_bin, zero_division=0)),\n",
    "    \"test_recall\": float(recall_score(test_trues_bin, test_preds_bin, zero_division=0)),\n",
    "    \"num_test_samples\": int(len(test_trues)),\n",
    "    \"inference_time_sec\": round(float(eval_time), 2)\n",
    "}\n",
    "\n",
    "# PhysioNet 2025 Challenge Score: fraction of true positives in top 5% ranked predictions\n",
    "sorted_idx = np.argsort(test_preds)[::-1]\n",
    "top_5_pct = int(0.05 * len(test_preds))\n",
    "positives_in_top = test_trues_bin[sorted_idx[:top_5_pct]].sum()\n",
    "challenge_score = positives_in_top / test_trues_bin.sum() if test_trues_bin.sum() > 0 else 0.0\n",
    "global_metrics[\"challenge_score_top5pct\"] = float(challenge_score)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"           FINAL TEST RESULTS (GLOBAL)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUROC           : {global_metrics['test_auc']:.4f}\")\n",
    "print(f\"AUPRC           : {global_metrics['test_auprc']:.4f}\")\n",
    "print(f\"Accuracy        : {global_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"F1 Score        : {global_metrics['test_f1']:.4f}\")\n",
    "print(f\"Precision       : {global_metrics['test_precision']:.4f}\")\n",
    "print(f\"Recall          : {global_metrics['test_recall']:.4f}\")\n",
    "print(f\"Challenge Score : {challenge_score:.4f} (top 5%)\")\n",
    "print(f\"Test Samples    : {global_metrics['num_test_samples']}\")\n",
    "print(f\"Inference Time  : {eval_time:.1f}s ({eval_time/len(test_trues)*1000:.1f} ms/sample)\")\n",
    "\n",
    "# -------------------------\n",
    "# Subgroup Analysis by Dataset (Critical for Viva & Generalization Discussion)\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"       SUBGROUP PERFORMANCE BY DATASET\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "subgroup_results = {}\n",
    "test_df_reset = test_df.reset_index(drop=True)\n",
    "\n",
    "for ds_name in test_df_reset[\"dataset\"].unique():\n",
    "    mask = test_df_reset[\"dataset\"] == ds_name\n",
    "    n_samples = mask.sum()\n",
    "    \n",
    "    if n_samples == 0:\n",
    "        continue\n",
    "    \n",
    "    ds_preds = test_preds[mask]\n",
    "    ds_trues_bin = test_trues_bin[mask]\n",
    "    \n",
    "    ds_auc = \"N/A\"\n",
    "    if len(np.unique(ds_trues_bin)) > 1:\n",
    "        ds_auc = roc_auc_score(ds_trues_bin, ds_preds)\n",
    "        subgroup_results[ds_name] = float(ds_auc)\n",
    "        print(f\"{ds_name:12} | n={n_samples:5d} | AUROC: {ds_auc:.4f}\")\n",
    "    else:\n",
    "        subgroup_results[ds_name] = None\n",
    "        print(f\"{ds_name:12} | n={n_samples:5d} | Only one class ‚Üí AUROC skipped\")\n",
    "\n",
    "# -------------------------\n",
    "# Save Complete Results\n",
    "# -------------------------\n",
    "final_results = {\n",
    "    \"global_metrics\": global_metrics,\n",
    "    \"subgroup_auroc\": subgroup_results,\n",
    "    \"model\": {\n",
    "        \"architecture\": \"ViT-Small\",\n",
    "        \"patch_size\": \"8x16 (rectangular)\",\n",
    "        \"embed_dim\": 384,\n",
    "        \"depth\": 12,\n",
    "        \"heads\": 6,\n",
    "        \"normalization\": \"Zero-centered [-1, 1] from uint8\",\n",
    "        \"total_parameters\": num_params\n",
    "    },\n",
    "    \"training_details\": {\n",
    "        \"effective_batch_size\": effective_batch_size,\n",
    "        \"mixed_precision\": use_amp,\n",
    "        \"gradient_accumulation\": accumulation_steps\n",
    "    },\n",
    "    \"date_completed\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "results_path = EXP_DIR / \"test_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nComprehensive test results saved to: {results_path}\")\n",
    "if device.type == \"cuda\":\n",
    "    final_mem = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "    print(f\"Final peak GPU memory during test: {final_mem:.2f} GB\")\n",
    "\n",
    "print(f\"‚è± Cell 6 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a72223",
   "metadata": {},
   "source": [
    "##### Cell 6 ‚Äî What this does\n",
    "- Loads the **best-performing checkpoint** (by validation AUROC).\n",
    "- Performs **final inference** on the held-out test set using mixed-precision and non-blocking transfers.\n",
    "- Computes **complete benchmark metrics**:\n",
    "  - AUROC, AUPRC, Accuracy, F1, Precision, Recall\n",
    "  - **Official Challenge Score** (fraction of true Chagas cases ranked in top 5%)\n",
    "- Provides **subgroup analysis** by original dataset (PTB-XL, SaMi-Trop, CODE-15%) ‚Äî essential for discussing generalization and domain shift.\n",
    "- Saves a **comprehensive, publication-ready** `test_results.json` with all metrics, model specs, and training details.\n",
    "\n",
    "##### Key Research Insights Enabled\n",
    "- Compare your global AUROC/AUPRC against Kim et al. (~0.50‚Äì0.55 range expected) and Van Santvliet et al.\n",
    "- Analyze subgroup gaps ‚Üí discuss limitations of European (PTB-XL) vs Latin American (SaMi-Trop/CODE-15) data.\n",
    "- Use Challenge Score as primary success metric for PhysioNet 2025 alignment.\n",
    "\n",
    "##### Future improvements\n",
    "- Add **bootstrapped confidence intervals** (95% CI) on AUROC for statistical rigor.\n",
    "- Compute **optimal threshold** via Youden‚Äôs J or precision-recall trade-off.\n",
    "- Export predictions for ensemble with 1D ECG-FM (future hybrid model).\n",
    "- Visualize high-attention patches via Grad-CAM or attention rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 7 ‚Äî Visual Debugger: RA/LA/LL Contours + Model Inference (RESEARCH-READY)\n",
    "# =========================\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def visualize_contours_and_predict(img_tensor: torch.Tensor, \n",
    "                                   true_label: float = None, \n",
    "                                   record_info: str = \"Unknown Record\",\n",
    "                                   prob: float = None):\n",
    "    \"\"\"\n",
    "    Visualizes the 3-channel RA/LA/LL contour embedding and reports model prediction.\n",
    "    \n",
    "    Aligns with Kim et al. (2025) physiological spatial representation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Model Inference\n",
    "    with torch.no_grad():\n",
    "        img_batch = img_tensor.unsqueeze(0).to(device, non_blocking=True)\n",
    "        logits = model(img_batch)\n",
    "        probability = torch.sigmoid(logits).item()\n",
    "\n",
    "    # 2. Denormalize for clean visualization [0, 255] ‚Üí uint8\n",
    "    display_img = ((img_tensor.cpu().numpy() + 1.0) * 127.5).astype(np.uint8)  # (3, 24, 2048)\n",
    "\n",
    "    # 3. Plot the three physiological channels\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 9), sharex=True)\n",
    "    fig.suptitle(f\"RA / LA / LL Contour Visualization ‚Äî {record_info}\\n\"\n",
    "                 f\"Model Confidence: {probability*100:.2f}% ‚Üí \"\n",
    "                 f\"{'POSITIVE (Chagas)' if probability > 0.5 else 'NEGATIVE'}\", \n",
    "                 fontsize=16, y=0.98)\n",
    "\n",
    "    channel_names = [\n",
    "        \"Right-Arm Reference Channel\\n(Leads: I, II, V1‚ÄìV6, aVL, aVR, aVF projected onto RA)\",\n",
    "        \"Left-Arm Reference Channel\\n(Leads: I, II, V1‚ÄìV6, aVR, aVF, aVL projected onto LA)\",\n",
    "        \"Left-Leg Reference Channel\\n(Leads: II, III, V1‚ÄìV6, aVF, aVL, aVR projected onto LL)\"\n",
    "    ]\n",
    "\n",
    "    for i in range(3):\n",
    "        im = axes[i].imshow(display_img[i], aspect='auto', cmap='viridis', vmin=0, vmax=255)\n",
    "        axes[i].set_title(channel_names[i], fontsize=12, pad=10)\n",
    "        axes[i].set_ylabel(\"Lead Group\\n(8 leads stacked)\", fontsize=10)\n",
    "        fig.colorbar(im, ax=axes[i], fraction=0.015, pad=0.02, label=\"Amplitude (original uint8 scale)\")\n",
    "\n",
    "    axes[2].set_xlabel(\"Time (2048 samples ‚âà 10 seconds at 500 Hz)\", fontsize=12)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Detailed Inference Report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"        INFERENCE REPORT ‚Äî {record_info}\")\n",
    "    print(\"=\"*60)\n",
    "    if true_label is not None:\n",
    "        gt_class = \"POSITIVE (Chagas)\" if true_label > 0.5 else \"NEGATIVE\"\n",
    "        print(f\"Ground Truth Label : {gt_class} (soft label = {true_label:.3f})\")\n",
    "    \n",
    "    pred_class = \"POSITIVE (Chagas)\" if probability > 0.5 else \"NEGATIVE\"\n",
    "    print(f\"Model Prediction   : {pred_class}\")\n",
    "    print(f\"Confidence (Prob)  : {probability*100:.2f}%\")\n",
    "    \n",
    "    if true_label is not None:\n",
    "        correct = (probability > 0.5) == (true_label > 0.5)\n",
    "        print(f\"Prediction Correct : {'YES ‚úÖ' if correct else 'NO ‚ùå'}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# =========================\n",
    "# Interactive Debugging Modes\n",
    "# =========================\n",
    "\n",
    "# --- MODE 1: Manual Path Mode ---\n",
    "MANUAL_NPY_PATH = \"\"  # ‚Üê Set this to test any specific record, e.g.:\n",
    "# MANUAL_NPY_PATH = \"data/processed/2d_images/code15/1000010_img.npy\"\n",
    "# MANUAL_NPY_PATH = \"data/processed/2d_images/sami_trop/24028_img.npy\"\n",
    "\n",
    "if MANUAL_NPY_PATH:\n",
    "    manual_path = PROJECT_ROOT / Path(MANUAL_NPY_PATH.replace(\"\\\\\", \"/\"))\n",
    "    if manual_path.exists():\n",
    "        print(f\"Loading manual record: {manual_path.name}\")\n",
    "        raw_img = np.load(manual_path).astype(np.float32)\n",
    "        norm_img = torch.from_numpy((raw_img / 127.5) - 1.0)\n",
    "        visualize_contours_and_predict(norm_img, record_info=f\"Manual: {manual_path.name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {manual_path}\")\n",
    "\n",
    "# --- MODE 2: Random Test Set Sample (Default) ---\n",
    "else:\n",
    "    print(\"Selecting random record from held-out test set for debugging...\")\n",
    "    idx = np.random.randint(0, len(test_ds))\n",
    "    img_tensor, true_label = test_ds[idx]\n",
    "    \n",
    "    # Optional: retrieve metadata for richer title\n",
    "    record_row = test_df.iloc[idx]\n",
    "    record_id = record_row.get(\"exam_id\", record_row.get(\"ecg_id\", idx))\n",
    "    dataset_name = record_row[\"dataset\"].upper()\n",
    "    \n",
    "    record_info = f\"Test Set #{idx} | ID: {record_id} | Dataset: {dataset_name}\"\n",
    "    \n",
    "    visualize_contours_and_predict(\n",
    "        img_tensor, \n",
    "        true_label=true_label.item(), \n",
    "        record_info=record_info\n",
    "    )\n",
    "\n",
    "print(f\"‚è± Cell 7 execution time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8180e6b",
   "metadata": {},
   "source": [
    "##### Cell 7 ‚Äî Visual Debugger: RA / LA / LL Physiological Contours + Live Inference\n",
    "- Loads a single 2D contour image (either manually specified or randomly from the held-out test set).\n",
    "- **Visualizes the three physiologically structured channels** exactly as defined in Kim et al. (2025):\n",
    "  - Right-Arm, Left-Arm, and Left-Leg reference projections\n",
    "  - Each channel stacks 8 leads vertically for spatial preservation\n",
    "- Runs **live inference** using the trained ViT-Small model.\n",
    "- Reports:\n",
    "  - Model confidence and binary decision\n",
    "  - Ground truth (if available)\n",
    "  - Correctness of prediction\n",
    "- Ideal for:\n",
    "  - Understanding model decisions on individual cases\n",
    "  - Demonstrating spatial embedding during viva/project presentation\n",
    "  - Debugging false positives/negatives across datasets\n",
    "\n",
    "##### Key Research Value\n",
    "This visualization directly shows why rectangular patches and zero-centered normalization matter ‚Äî subtle conduction abnormalities in Chagas disease become spatially organized across the three reference views."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
