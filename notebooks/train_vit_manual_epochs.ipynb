{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f9010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "VRAM (GB): 6.441926656\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA version: 11.8\n",
      "PROJECT_ROOT: d:\\IIT\\L6\\FYP\\ChagaSight\n",
      "DATA_DIR: d:\\IIT\\L6\\FYP\\ChagaSight\\data\\processed\n",
      "Experiment directory: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_contour_baseline\\20251231_181422\n",
      "‚è± Cell 1 time: 0.03s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 1 (Code) ‚Äî Setup, device, paths, seed + GPU Monitoring\n",
    "# =========================\n",
    "\n",
    "import time, random, sys\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", torch.cuda.get_device_properties(0).total_memory / 1e9)\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "# GPU monitoring\n",
    "def monitor_gpu():\n",
    "    try:\n",
    "        subprocess.Popen([\n",
    "            'nvidia-smi',\n",
    "            '--query-gpu=timestamp,name,utilization.gpu,memory.used,memory.total',\n",
    "            '--format=csv',\n",
    "            '-l', '5'\n",
    "        ])\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: nvidia-smi not found ‚Äì GPU monitoring skipped.\")\n",
    "\n",
    "threading.Thread(target=monitor_gpu, daemon=True).start()\n",
    "\n",
    "# Paths\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Experiment\n",
    "EXP_NAME = \"vit_contour_baseline\"\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = PROJECT_ROOT / \"experiments\" / EXP_NAME / RUN_ID\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Experiment directory:\", EXP_DIR)\n",
    "\n",
    "print(f\"‚è± Cell 1 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f30f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total metadata rows (raw): 63228\n",
      "Rows after integrity filter: 63228\n",
      "Subset records (100%): 63228\n",
      "Train: 50582 | Val: 6323 | Test: 6323\n",
      "‚è± Cell 2 time: 3.61s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 2 (Code) ‚Äî Metadata loading + integrity filtering + subset + splits\n",
    "# =========================\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "datasets = [\"ptbxl\", \"sami_trop\", \"code15\"]\n",
    "dfs = []\n",
    "\n",
    "for ds in datasets:\n",
    "    csv_path = DATA_DIR / \"metadata\" / f\"{ds}_metadata.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing metadata CSV: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"dataset\"] = ds\n",
    "    \n",
    "    df[\"label\"] = df[\"label\"].astype(float)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "df_all = df_all.dropna(subset=['label'])  # Drop any NaN labels\n",
    "print(\"Total metadata rows (raw):\", len(df_all))\n",
    "\n",
    "def img_exists(p):\n",
    "    clean_path = str(p).replace(\"\\\\\", \"/\")\n",
    "    return (PROJECT_ROOT / Path(clean_path)).exists()\n",
    "\n",
    "exists_mask = df_all[\"img_path\"].apply(img_exists)\n",
    "missing_count = (~exists_mask).sum()\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Dropping {missing_count} rows with missing image files\")\n",
    "    print(df_all.loc[~exists_mask, [\"dataset\", \"img_path\"]].head())\n",
    "\n",
    "df_all = df_all.loc[exists_mask].reset_index(drop=True)\n",
    "print(\"Rows after integrity filter:\", len(df_all))\n",
    "\n",
    "subset_frac = 1.0\n",
    "\n",
    "if subset_frac < 1.0:\n",
    "    df_all = df_all.sample(frac=subset_frac, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Subset records ({subset_frac*100:.0f}%):\", len(df_all))\n",
    "\n",
    "df_all[\"label_bin\"] = (df_all[\"label\"] > 0.5).astype(int)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.2,\n",
    "    stratify=df_all[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"‚è± Cell 2 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116e5724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoaders initialized ‚Äî Windows-safe, instant startup\n",
      "   Train batches per epoch: 3162\n",
      "   Val   batches per epoch: 396\n",
      "   Test  batches per epoch: 396\n",
      "‚úì Batch shape: torch.Size([16, 3, 24, 2048])\n",
      "‚úì Image range: -1.000 to 1.000\n",
      "‚úì Sample labels: [0.0, 0.20000000298023224, 0.0, 0.20000000298023224, 0.0, 0.20000000298023224, 0.0, 0.20000000298023224]\n",
      "‚è± Cell 3 time: 10.95s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 ‚Äî Dataset + DataLoaders (WINDOWS-SAFE, INSTANT INIT, FULL GPU SPEED)\n",
    "# =========================\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class ECGImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Fast, memory-efficient dataset for 2D contour images.\n",
    "    Designed for Windows + RTX 3050: no multiprocessing delay.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        # Pre-resolve paths once\n",
    "        self.img_paths = [(PROJECT_ROOT / Path(str(p))).resolve() for p in self.df[\"img_path\"]]\n",
    "        self.labels = self.df[\"label\"].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fast load (no mmap needed with num_workers=0)\n",
    "        img = np.load(self.img_paths[idx]).astype(np.float32)\n",
    "\n",
    "        # Shape validation (critical for research)\n",
    "        if img.shape != (3, 24, 2048):\n",
    "            raise ValueError(f\"Invalid shape {img.shape} at {self.img_paths[idx]}\")\n",
    "\n",
    "        # Zero-centered normalization [-1, 1] ‚Äî essential for ViT stability\n",
    "        img = (img / 127.5) - 1.0\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# -----------------------------\n",
    "# DataLoaders ‚Äî Windows-Optimized\n",
    "# -----------------------------\n",
    "batch_size = 16\n",
    "\n",
    "train_ds = ECGImageDataset(train_df)\n",
    "val_ds   = ECGImageDataset(val_df)\n",
    "test_ds  = ECGImageDataset(test_df)\n",
    "\n",
    "# Windows-safe: num_workers=0 avoids spawn delay\n",
    "# Speed comes from non_blocking + pin_memory in training loop\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,   # Avoids Windows pin_memory bug\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DataLoaders initialized ‚Äî Windows-safe, instant startup\")\n",
    "print(f\"   Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"   Val   batches per epoch: {len(val_loader)}\")\n",
    "print(f\"   Test  batches per epoch: {len(test_loader)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Sanity Check\n",
    "# -----------------------------\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"‚úì Batch shape: {x_batch.shape}\")                    # [16, 3, 24, 2048]\n",
    "print(f\"‚úì Image range: {x_batch.min().item():.3f} to {x_batch.max().item():.3f}\")  # ~ -1.0 to 1.0\n",
    "print(f\"‚úì Sample labels: {y_batch[:8].tolist()}\")\n",
    "\n",
    "print(f\"‚è± Cell 3 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93e71d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 38,224,897\n",
      "‚è± Cell 4 time: 0.21s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 4 (Code) ‚Äî ViTClassifier with Aggregation of Layers (AoL) + Upgraded Params\n",
    "# =========================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, patch_h=8, patch_w=16, embed_dim=512, depth=12, heads=8, mlp_ratio=4.0, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Patch embedding (for 3x24x2048 ‚Üí patches of 3x8x16)\n",
    "        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))\n",
    "        \n",
    "        # Position embedding + CLS token\n",
    "        num_patches = (24 // patch_h) * (2048 // patch_w)  # 3x128 patches\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Transformer blocks (ViT encoder)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, nhead=heads, dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "                dropout=dropout, activation='gelu'\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head with dropout\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        # x: (B,3,24,2048)\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embed: (B, embed_dim, 3, 128) ‚Üí flatten to (B, 384, embed_dim)\n",
    "        x = self.patch_embed(x)  # (B, embed_dim, H/p_h, W/p_w) = (B,512,3,128)\n",
    "        x = x.flatten(2).transpose(1,2)  # (B, 3*128=384, embed_dim)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B,385,embed_dim)\n",
    "        \n",
    "        # Add pos embed\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer blocks - Collect all intermediate layers for AoL (Van Santvliet)\n",
    "        layer_outputs = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            layer_outputs.append(x[:, 0])  # CLS token from each layer\n",
    "        \n",
    "        # AoL: Average pool all layer CLS tokens\n",
    "        feats = torch.stack(layer_outputs, dim=1).mean(dim=1)  # (B, embed_dim)\n",
    "        x = self.norm(feats)\n",
    "        \n",
    "        if return_feats:\n",
    "            return x  # For alignment/FM fusion\n",
    "        \n",
    "        return self.head(x)  # (B,1)\n",
    "\n",
    "# Initialize model (upgraded params for high accuracy)\n",
    "model = ViTClassifier().to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "print(f\"‚è± Cell 4 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd408c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... AMP: True | Effective batch: 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63807749dcfb4e54844dcde69af776e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 01:   0%|          | 0/3162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 5 (Code) ‚Äî Training Loop with All Improvements\n",
    "# =========================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Hyperparameters (Tuned for High Accuracy)\n",
    "# -------------------------\n",
    "num_epochs = 10\n",
    "lr = 2e-5  # From papers\n",
    "weight_decay = 0.01\n",
    "accumulation_steps = 2  # Effective batch=32\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "# -------------------------\n",
    "# Asymmetric/Weighted BCE Loss (Kim et al.)\n",
    "# -------------------------\n",
    "class AsymmetricBCE(nn.Module):\n",
    "    def __init__(self, gamma_pos=0, gamma_neg=2, pos_weight=10.0):\n",
    "        super().__init__()\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        bce = -(self.pos_weight * targets * torch.log(probs + 1e-8) * (1 - probs).pow(self.gamma_pos) +\n",
    "                (1 - targets) * torch.log(1 - probs + 1e-8) * probs.pow(self.gamma_neg))\n",
    "        return bce.mean()\n",
    "\n",
    "criterion = AsymmetricBCE(pos_weight=10.0)  # Tune pos_weight 5-15\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer + Warmup Scheduler (Van Santvliet Style)\n",
    "# -------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "warmup_epochs = 2\n",
    "def warmup_lambda(epoch): return (epoch + 1) / warmup_epochs if epoch < warmup_epochs else 1.0\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=lr/10)\n",
    "scheduler = SequentialLR(optimizer, [warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "\n",
    "# -------------------------\n",
    "# Early Stopping\n",
    "# -------------------------\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "best_val_auc = 0.0\n",
    "\n",
    "# -------------------------\n",
    "# Mixup Function (Kim et al. - Adapted for Images)\n",
    "# -------------------------\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "history = {'epoch': [], 'train_loss': [], 'val_auc': [], 'val_auprc': [], 'val_f1': [], 'challenge_score': []}\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "\n",
    "print(f\"Starting training... AMP: {use_amp} | Effective batch: {batch_size * accumulation_steps}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1:02d}\", leave=False)\n",
    "    for step, (imgs, labels) in enumerate(train_bar):\n",
    "        imgs, labels = imgs.to(device), labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        # Mixup (50% prob)\n",
    "        if random.random() < 0.5:\n",
    "            imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "                logits = model(imgs)\n",
    "                loss = lam * criterion(logits, labels_a) + (1 - lam) * criterion(logits, labels_b)\n",
    "                loss = loss / accumulation_steps\n",
    "        else:\n",
    "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits, labels) / accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * accumulation_steps\n",
    "        train_bar.set_postfix(loss=train_loss / (step + 1))\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "                logits = model(imgs)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "            val_preds.extend(probs)\n",
    "            val_trues.extend(labels.numpy())\n",
    "    \n",
    "    val_preds = np.array(val_preds)\n",
    "    val_trues_bin = (np.array(val_trues) > 0.5).astype(int)\n",
    "    val_preds_bin = (val_preds >= 0.5).astype(int)\n",
    "    \n",
    "    val_auc = roc_auc_score(val_trues_bin, val_preds)\n",
    "    val_auprc = average_precision_score(val_trues_bin, val_preds)\n",
    "    val_f1 = f1_score(val_trues_bin, val_preds_bin)\n",
    "    \n",
    "    # Challenge score\n",
    "    sorted_idx = np.argsort(val_preds)[::-1]\n",
    "    top_5_pct = int(0.05 * len(val_preds))\n",
    "    challenge_score = val_trues_bin[sorted_idx[:top_5_pct]].sum() / val_trues_bin.sum() if val_trues_bin.sum() > 0 else 0.0\n",
    "    \n",
    "    improved = \"\"\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        patience_counter = 0\n",
    "        torch.save({'model_state_dict': model.state_dict(), 'val_auc': val_auc, 'epoch': epoch + 1}, best_model_path)\n",
    "        improved = \"‚úÖ\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"‚ö†Ô∏è Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['epoch'].append(epoch + 1)\n",
    "    history['train_loss'].append(train_loss / len(train_loader))\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_auprc'].append(val_auprc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['challenge_score'].append(challenge_score)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {history['train_loss'][-1]:.4f} | AUROC: {val_auc:.4f} | AUPRC: {val_auprc:.4f} | F1: {val_f1:.4f} | Challenge: {challenge_score:.4f} {improved} | Time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# Save history\n",
    "pd.DataFrame(history).to_csv(EXP_DIR / \"metrics.csv\", index=False)\n",
    "\n",
    "# Plot curves\n",
    "df_history = pd.DataFrame(history)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 4))\n",
    "axes[0].plot(df_history['epoch'], df_history['train_loss']); axes[0].set_title('Train Loss'); axes[0].grid(True)\n",
    "axes[1].plot(df_history['epoch'], df_history['val_auc']); axes[1].axhline(0.85, color='r', ls='--'); axes[1].set_title('Val AUROC'); axes[1].grid(True)\n",
    "axes[2].plot(df_history['epoch'], df_history['val_auprc']); axes[2].set_title('Val AUPRC'); axes[2].grid(True)\n",
    "axes[3].plot(df_history['epoch'], df_history['challenge_score']); axes[3].axhline(0.45, color='r', ls='--'); axes[3].set_title('Challenge Score'); axes[3].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EXP_DIR / 'training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéâ Training complete! Best AUROC: {best_val_auc:.4f}\")\n",
    "print(f\"‚è± Cell 5 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 6 ‚Äî Final Test Evaluation + Comprehensive Reporting (RESEARCH-READY)\n",
    "# =========================\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, accuracy_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Load Best Model Checkpoint (FIXED LOADING LOGIC)\n",
    "# -------------------------\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "if not best_model_path.exists():\n",
    "    raise FileNotFoundError(f\"Best model not found at {best_model_path}\")\n",
    "\n",
    "checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Correct loading: Extract 'model_state_dict' if it's a dict\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint.get('epoch', 'unknown')} \"\n",
    "          f\"with val_auc {checkpoint.get('val_auc', 'N/A'):.4f}\")\n",
    "else:\n",
    "    # Fallback for flat state_dict (if you saved without dict)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"Loaded model weights (no metadata available)\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# Full Inference on Test Set\n",
    "# -------------------------\n",
    "test_preds = []\n",
    "test_trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Final Test Inference\", leave=False):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "\n",
    "        test_preds.extend(probs.tolist() if probs.ndim > 0 else [float(probs)])\n",
    "        test_trues.extend(labels.numpy().tolist())\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "test_trues = np.array(test_trues)\n",
    "test_trues_bin = (test_trues > 0.5).astype(int)  # Binarize soft labels for metrics\n",
    "test_preds_bin = (test_preds >= 0.5).astype(int)\n",
    "\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "# -------------------------\n",
    "# Global Metrics (Full Benchmark Set)\n",
    "# -------------------------\n",
    "global_metrics = {\n",
    "    \"test_auc\": float(roc_auc_score(test_trues_bin, test_preds)),\n",
    "    \"test_auprc\": float(average_precision_score(test_trues_bin, test_preds)),\n",
    "    \"test_accuracy\": float(accuracy_score(test_trues_bin, test_preds_bin)),\n",
    "    \"test_f1\": float(f1_score(test_trues_bin, test_preds_bin)),\n",
    "    \"test_precision\": float(precision_score(test_trues_bin, test_preds_bin, zero_division=0)),\n",
    "    \"test_recall\": float(recall_score(test_trues_bin, test_preds_bin, zero_division=0)),\n",
    "    \"num_test_samples\": int(len(test_trues)),\n",
    "    \"inference_time_sec\": round(float(eval_time), 2)\n",
    "}\n",
    "\n",
    "# PhysioNet 2025 Challenge Score: fraction of true positives in top 5% ranked predictions\n",
    "sorted_idx = np.argsort(test_preds)[::-1]\n",
    "top_5_pct = int(0.05 * len(test_preds))\n",
    "positives_in_top = test_trues_bin[sorted_idx[:top_5_pct]].sum()\n",
    "challenge_score = positives_in_top / test_trues_bin.sum() if test_trues_bin.sum() > 0 else 0.0\n",
    "global_metrics[\"challenge_score_top5pct\"] = float(challenge_score)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"           FINAL TEST RESULTS (GLOBAL)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUROC           : {global_metrics['test_auc']:.4f}\")\n",
    "print(f\"AUPRC           : {global_metrics['test_auprc']:.4f}\")\n",
    "print(f\"Accuracy        : {global_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"F1 Score        : {global_metrics['test_f1']:.4f}\")\n",
    "print(f\"Precision       : {global_metrics['test_precision']:.4f}\")\n",
    "print(f\"Recall          : {global_metrics['test_recall']:.4f}\")\n",
    "print(f\"Challenge Score : {challenge_score:.4f} (top 5%)\")\n",
    "print(f\"Test Samples    : {global_metrics['num_test_samples']}\")\n",
    "print(f\"Inference Time  : {eval_time:.1f}s ({eval_time/len(test_trues)*1000:.1f} ms/sample)\")\n",
    "\n",
    "# -------------------------\n",
    "# Subgroup Analysis by Dataset (Critical for High Accuracy & Generalization)\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"       SUBGROUP PERFORMANCE BY DATASET\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "subgroup_results = {}\n",
    "test_df_reset = test_df.reset_index(drop=True)  # Align indices with arrays\n",
    "\n",
    "for ds_name in test_df_reset[\"dataset\"].unique():\n",
    "    mask = test_df_reset[\"dataset\"] == ds_name\n",
    "    n_samples = mask.sum()\n",
    "    \n",
    "    if n_samples == 0:\n",
    "        continue\n",
    "    \n",
    "    ds_preds = test_preds[mask]\n",
    "    ds_trues_bin = test_trues_bin[mask]\n",
    "    \n",
    "    ds_auc = \"N/A\"\n",
    "    if len(np.unique(ds_trues_bin)) > 1:  # Skip if only one class (e.g., all negatives in PTB-XL)\n",
    "        ds_auc = roc_auc_score(ds_trues_bin, ds_preds)\n",
    "        subgroup_results[ds_name] = float(ds_auc)\n",
    "        print(f\"{ds_name:12} | n={n_samples:5d} | AUROC: {ds_auc:.4f}\")\n",
    "    else:\n",
    "        subgroup_results[ds_name] = None\n",
    "        print(f\"{ds_name:12} | n={n_samples:5d} | Only one class ‚Üí AUROC skipped\")\n",
    "\n",
    "# -------------------------\n",
    "# Save Complete Results (For Project Report/Viva)\n",
    "# -------------------------\n",
    "final_results = {\n",
    "    \"global_metrics\": global_metrics,\n",
    "    \"subgroup_auroc\": subgroup_results,\n",
    "    \"model\": {\n",
    "        \"architecture\": \"ViT-Small\",\n",
    "        \"patch_size\": \"8x16 (rectangular)\",\n",
    "        \"embed_dim\": 384,\n",
    "        \"depth\": 12,\n",
    "        \"heads\": 6,\n",
    "        \"normalization\": \"Zero-centered [-1, 1] from uint8\",\n",
    "        \"total_parameters\": sum(p.numel() for p in model.parameters())\n",
    "    },\n",
    "    \"training_details\": {\n",
    "        \"effective_batch_size\": batch_size * accumulation_steps,\n",
    "        \"mixed_precision\": use_amp,\n",
    "        \"gradient_accumulation\": accumulation_steps\n",
    "    },\n",
    "    \"date_completed\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "results_path = EXP_DIR / \"test_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nComprehensive test results saved to: {results_path}\")\n",
    "if device.type == \"cuda\":\n",
    "    final_mem = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "    print(f\"Final peak GPU memory during test: {final_mem:.2f} GB\")\n",
    "\n",
    "print(f\"‚è± Cell 6 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 7 (Code) ‚Äî Basic ECG Foundation Model on 100Hz 1D Signals\n",
    "# =========================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class ECG1DDataset(Dataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        self.signal_paths = df['path_100hz'].tolist()  # Add 'path_100hz' to df from metadata\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signal_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = np.load(self.signal_paths[idx]).astype(np.float32)  # (12,1000)\n",
    "        if self.augment:\n",
    "            # Masking for self-supervised (ST-MEM: 75% random mask)\n",
    "            mask = np.random.rand(1000) < 0.75\n",
    "            signal[:, mask] = 0  # Mask time points across all leads\n",
    "        \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return torch.from_numpy(signal), label\n",
    "\n",
    "# FM Model (ViT 1D with decoder for pretraining)\n",
    "class ECGFM(nn.Module):\n",
    "    def __init__(self, patch_size=50, embed_dim=768, depth=12, heads=12):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv1d(12, embed_dim, kernel_size=patch_size, stride=patch_size)  # Patches: 1000/50=20\n",
    "        num_patches = 20\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, heads, dim_feedforward=embed_dim*4, dropout=0.1)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 12 * patch_size),  # Reconstruct patch\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (B, embed_dim, 20)\n",
    "        x = x.transpose(1,2)  # (B,20,embed_dim)\n",
    "        \n",
    "        cls = self.cls_token.expand(B,-1,-1)\n",
    "        x = torch.cat([cls, x], dim=1) + self.pos_embed\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        feats = x[:,0]  # CLS\n",
    "        \n",
    "        # Decode for pretraining\n",
    "        recon = self.decoder(x[:,1:]).view(B, -1, 12, 50).transpose(2,3).reshape(B,12,1000)  # Reconstruct\n",
    "        \n",
    "        return feats, recon\n",
    "\n",
    "# Pretrain FM (Self-Supervised: MSE on masked reconstruction)\n",
    "fm_model = ECGFM().to(device)\n",
    "fm_optimizer = AdamW(fm_model.parameters(), lr=1e-4)\n",
    "fm_criterion = nn.MSELoss()\n",
    "\n",
    "# Create 1D datasets (aug=True for masking)\n",
    "train_1d_ds = ECG1DDataset(train_df, augment=True)\n",
    "train_1d_loader = DataLoader(train_1d_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "print(\"Pretraining FM...\")\n",
    "for epoch in range(10):  # Short pretrain\n",
    "    fm_model.train()\n",
    "    fm_loss = 0.0\n",
    "    for signals, _ in tqdm(train_1d_loader, leave=False):\n",
    "        signals = signals.to(device)\n",
    "        _, recon = fm_model(signals)\n",
    "        loss = fm_criterion(recon, signals)  # Reconstruct original\n",
    "        fm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        fm_optimizer.step()\n",
    "        fm_loss += loss.item()\n",
    "    print(f\"FM Epoch {epoch+1}: Loss {fm_loss / len(train_1d_loader):.4f}\")\n",
    "\n",
    "torch.save(fm_model.state_dict(), EXP_DIR / \"fm_pretrained.pth\")\n",
    "print(f\"‚è± Cell 7 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c410ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 8 (Code) ‚Äî Hybrid Training: ViT (2D) + FM (1D) with Cosine Alignment\n",
    "# =========================\n",
    "\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load pretrained FM\n",
    "fm_model = ECGFM().to(device)\n",
    "fm_model.load_state_dict(torch.load(EXP_DIR / \"fm_pretrained.pth\"))\n",
    "fm_model.eval()  # Freeze for alignment\n",
    "\n",
    "# Update ViT to return features\n",
    "# In Cell 4 forward: add if return_feats: return feats  (already in improved Cell 4)\n",
    "\n",
    "# Hybrid loss weight\n",
    "alignment_weight = 0.5  # From Kim et al.\n",
    "\n",
    "# New datasets: Load both 2D + 1D\n",
    "class HybridDataset(ECGImageDataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        super().__init__(df, augment)\n",
    "        self.signal_paths = df['path_100hz'].tolist()  # Assume added to df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = super().__getitem__(idx)\n",
    "        signal = torch.from_numpy(np.load(self.signal_paths[idx]).astype(np.float32))\n",
    "        return img, signal, label\n",
    "\n",
    "train_hybrid_ds = HybridDataset(train_df, augment=True)\n",
    "train_hybrid_loader = DataLoader(train_hybrid_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "# Hybrid training loop (similar to Cell 5, but with both inputs)\n",
    "print(\"Hybrid Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for imgs, signals, labels in tqdm(train_hybrid_loader, leave=False):\n",
    "        imgs, signals, labels = imgs.to(device), signals.to(device), labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        # Mixup on imgs (optional on signals)\n",
    "        if random.random() < 0.5:\n",
    "            imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "            vit_feats = model(imgs, return_feats=True)\n",
    "            _, fm_recon = fm_model(signals)  # Ignore recon\n",
    "            fm_feats = fm_model(signals)[0]  # Feats\n",
    "            bce = lam * criterion(model.head(vit_feats), labels_a) + (1 - lam) * criterion(model.head(vit_feats), labels_b)\n",
    "            cos_loss = 1 - F.cosine_similarity(vit_feats, fm_feats).mean()\n",
    "            loss = bce + alignment_weight * cos_loss\n",
    "        else:\n",
    "            vit_feats = model(imgs, return_feats=True)\n",
    "            fm_feats = fm_model(signals)[0]\n",
    "            bce = criterion(model.head(vit_feats), labels)\n",
    "            cos_loss = 1 - F.cosine_similarity(vit_feats, fm_feats).mean()\n",
    "            loss = bce + alignment_weight * cos_loss\n",
    "        \n",
    "        loss = loss / accumulation_steps\n",
    "        # Backward/scaler as before...\n",
    "    \n",
    "    # Validation/evaluation as in Cell 5 (use hybrid val loader similarly)\n",
    "    # ...\n",
    "\n",
    "print(f\"‚è± Cell 8 time: {time.time() - start_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
