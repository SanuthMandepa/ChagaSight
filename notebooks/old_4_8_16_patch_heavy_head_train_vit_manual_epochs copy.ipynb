{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612a80f4",
   "metadata": {},
   "source": [
    "### ChagaSight ‚Äî Vision Transformer (Baseline Training)\n",
    "\n",
    "Baseline ViT training on 2D ECG contour images  \n",
    "Datasets: PTB-XL (negatives), SaMi-Trop (positives), CODE-15 (soft labels)\n",
    "\n",
    "Baseline configuration:\n",
    "- 1% subset (pipeline verification)\n",
    "- No data augmentation\n",
    "- AMP enabled\n",
    "- Strict data integrity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f368854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "VRAM (GB): 6.441926656\n",
      "PROJECT_ROOT: d:\\IIT\\L6\\FYP\\ChagaSight\n",
      "DATA_DIR: d:\\IIT\\L6\\FYP\\ChagaSight\\data\\processed\n",
      "Experiment directory: d:\\IIT\\L6\\FYP\\ChagaSight\\experiments\\vit_contour_baseline\\20251227_021046\n",
      "‚è± Cell 1 time: 0.03s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 1 (Code) ‚Äî Setup, device, paths, seed\n",
    "# =========================\n",
    "\n",
    "import time, random, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility (Baseline-Safe)\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Deterministic baseline (Crucial for medical AI research)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Device Configuration\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", torch.cuda.get_device_properties(0).total_memory / 1e9)\n",
    "\n",
    "# -------------------------\n",
    "# Universal Project Root Detection\n",
    "# -------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"Finds the directory containing the 'data' folder.\"\"\"\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "# Set current working directory based on the notebook location\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Add project root to sys.path for importing local src modules\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Experiment Management\n",
    "# -------------------------\n",
    "# Final year project name aligned with research papers\n",
    "EXP_NAME = \"vit_contour_baseline\" \n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = PROJECT_ROOT / \"experiments\" / EXP_NAME / RUN_ID\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Experiment directory:\", EXP_DIR)\n",
    "\n",
    "print(f\"‚è± Cell 1 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa73d6",
   "metadata": {},
   "source": [
    "#### Cell 1 ‚Äî What this does\n",
    "- Fixes randomness using a seed for reproducibility.\n",
    "- Detects GPU and prints VRAM.\n",
    "- Finds the project root robustly (works in VS Code even if the notebook is in `/notebooks`).\n",
    "- Creates a unique experiment run folder under `experiments/<EXP_NAME>/<RUN_ID>/`.\n",
    "\n",
    "#### Future improvements\n",
    "- Run multiple seeds (e.g., 5 runs) and report mean ¬± std AUROC.\n",
    "- Log CUDA + PyTorch versions for stronger reproducibility.\n",
    "- For speed-focused runs (not baseline), enable `torch.backends.cudnn.benchmark = True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71329e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total metadata rows (raw): 63228\n",
      "Rows after integrity filter: 63228\n",
      "Subset records (100%): 63228\n",
      "Train: 50582 | Val: 6323 | Test: 6323\n",
      "‚è± Cell 2 time: 10.08s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 2 (Code) ‚Äî Metadata loading + integrity filtering + subset + splits\n",
    "# =========================\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Datasets included (Kim et al. & Van Santvliet et al. benchmarks)\n",
    "# -------------------------\n",
    "datasets = [\"ptbxl\", \"sami_trop\", \"code15\"]\n",
    "dfs = []\n",
    "\n",
    "for ds in datasets:\n",
    "    csv_path = DATA_DIR / \"metadata\" / f\"{ds}_metadata.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing metadata CSV: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"dataset\"] = ds\n",
    "    \n",
    "    # Ensure labels are floats for soft-label support (0.2 / 0.8)\n",
    "    df[\"label\"] = df[\"label\"].astype(float)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all datasets into a unified research corpus\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Total metadata rows (raw):\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# HARD Integrity Filter (Cross-platform path safe)\n",
    "# -------------------------\n",
    "def img_exists(p):\n",
    "    # Normalize Windows backslashes to universal forward slashes\n",
    "    clean_path = str(p).replace(\"\\\\\", \"/\")\n",
    "    return (PROJECT_ROOT / Path(clean_path)).exists()\n",
    "\n",
    "exists_mask = df_all[\"img_path\"].apply(img_exists)\n",
    "missing_count = (~exists_mask).sum()\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Dropping {missing_count} rows with missing image files\")\n",
    "    print(df_all.loc[~exists_mask, [\"dataset\", \"img_path\"]].head())\n",
    "\n",
    "df_all = df_all.loc[exists_mask].reset_index(drop=True)\n",
    "print(\"Rows after integrity filter:\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# Subset Control\n",
    "# -------------------------\n",
    "# Research Note: Use 1.0 for final results, 0.01 for code verification\n",
    "subset_frac = 1.0\n",
    "\n",
    "if subset_frac < 1.0:\n",
    "    df_all = df_all.sample(frac=subset_frac, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Subset records ({subset_frac*100:.0f}%):\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# Binary Stratification (Clinical Ground Truth)\n",
    "# -------------------------\n",
    "# We use binary labels ONLY for splitting to ensure class balance across sets.\n",
    "# 0.5 threshold used to binarize SaMi-Trop (1.0) and CODE-15% soft labels (0.8).\n",
    "df_all[\"label_bin\"] = (df_all[\"label\"] > 0.5).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Val / Test split (80 / 10 / 10)\n",
    "# -------------------------\n",
    "# Stratification ensures the endemic Chagas prevalence is consistent across splits.\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.2,\n",
    "    stratify=df_all[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"‚è± Cell 2 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc685d",
   "metadata": {},
   "source": [
    "#### Cell 2 ‚Äî What this does\n",
    "- Loads metadata CSVs for PTB-XL, SaMi-Trop, and CODE-15.\n",
    "- Drops any rows where the `.npy` image file is missing (prevents DataLoader crashes).\n",
    "- Samples a 1% subset for fast validation that the pipeline is correct.\n",
    "- Creates stratified Train/Val/Test splits using a binary label for metrics only.\n",
    "\n",
    "#### Future improvements\n",
    "- Scale from `subset_frac=0.01` ‚Üí `0.10` ‚Üí `1.0` once stable.\n",
    "- Consider dataset-aware splits (e.g., holding out one dataset for domain generalisation testing).\n",
    "- If patient IDs exist, enforce patient-wise splitting to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a741ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full dataset training (natural distribution)\n",
      "‚úì Batch shape: torch.Size([16, 3, 24, 2048])\n",
      "‚úì Image range: -1.00 to 1.00\n",
      "‚úì Sample labels: [0.0, 0.20000000298023224, 0.0, 0.20000000298023224, 0.0]\n",
      "‚è± Cell 3 time: 0.28s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 ‚Äî Dataset + DataLoaders (FINAL, research-correct)\n",
    "# =========================\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class ECGImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for 2D ECG image embeddings (Kim et al. 2025).\n",
    "    \n",
    "    Labels are SOFT labels used for weak supervision:\n",
    "    - PTB-XL: 0.0 (definite negative)\n",
    "    - SaMi-Trop: 1.0 (definite positive)\n",
    "    - CODE-15: soft uncertainty labels (e.g., 0.2 / 0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Cross-platform path handling\n",
    "        img_path = PROJECT_ROOT / Path(str(row[\"img_path\"]).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image file: {img_path}\")\n",
    "\n",
    "        # Loading the 2D contour image (C, H, W) = (3, 24, 2048)\n",
    "        img = np.load(img_path).astype(np.float32)\n",
    "\n",
    "        # Integrity check for research consistency\n",
    "        if img.shape != (3, 24, 2048):\n",
    "            raise ValueError(f\"Invalid image shape {img.shape} at {img_path}\")\n",
    "\n",
    "        # üî¥ RESEARCH FIX: Zero-centered normalization\n",
    "        # We scale from [0, 255] to [-1, 1]. \n",
    "        # This ensures the ECG baseline (127.5) maps to 0.0, \n",
    "        # which is crucial for self-attention stability.\n",
    "        img = (img / 127.5) - 1.0\n",
    "\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        # Soft labels are preserved for the BCEWithLogitsLoss\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Batch size optimized for RTX 3050 6GB\n",
    "batch_size = 16 \n",
    "\n",
    "train_ds = ECGImageDataset(train_df)\n",
    "val_ds   = ECGImageDataset(val_df)\n",
    "test_ds  = ECGImageDataset(test_df)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoader Configuration\n",
    "# -------------------------\n",
    "if subset_frac < 0.1:\n",
    "    print(\"‚ö†Ô∏è Oversampling enabled (debug subset mode)\")\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "    # Higher weights for Chagas positive/uncertain cases (SaMi-Trop and CODE-15 Pos)\n",
    "    weights = train_df[\"label\"].apply(lambda x: 10.0 if x > 0.7 else 1.0).values\n",
    "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ Full dataset training (natural distribution)\")\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# -------------------------\n",
    "# Sanity Check for Viva\n",
    "# -------------------------\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"‚úì Batch shape: {x_batch.shape}\") # Expect [16, 3, 24, 2048]\n",
    "print(f\"‚úì Image range: {x_batch.min().item():.2f} to {x_batch.max().item():.2f}\") # Expect approx -1.0 to 1.0\n",
    "print(f\"‚úì Sample labels: {y_batch[:5].tolist()}\")\n",
    "\n",
    "print(f\"‚è± Cell 3 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b1f09",
   "metadata": {},
   "source": [
    "### Why image normalisation is required\n",
    "Vision Transformers are sensitive to input scale because attention scores\n",
    "are computed directly from dot products. Normalising ECG images to [0,1]\n",
    "ensures stable optimisation and fair comparison across datasets.\n",
    "\n",
    "### Why soft labels are used\n",
    "CODE-15 annotations reflect uncertainty rather than binary truth.\n",
    "Soft-label training enables weak supervision and avoids forcing noisy\n",
    "labels into hard categories, aligning with the referenced ECG foundation\n",
    "model literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84669f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT trainable parameters: 85,648,897\n",
      "‚úì Forward OK | logits shape: torch.Size([16])\n",
      "‚úì Peak GPU memory used (GB): 0.74\n",
      "‚è± Cell 4 time: 1.37s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4 ‚Äî ViT model + forward sanity + peak memory (FINAL)\n",
    "# =========================\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_h=8, patch_w=16, in_ch=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        # üî¥ CRITICAL RESEARCH FIX:\n",
    "        # Use rectangular patches to perfectly tile the 24x2048 image.\n",
    "        # This prevents the 33% data loss occurring with square 16x16 patches.\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))\n",
    "        \n",
    "        # Updated calculation: (24/8) * (2048/16) = 3 * 128 = 384 patches\n",
    "        self.num_patches = (24 // patch_h) * (2048 // patch_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                  # (B, E, 3, 128)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, 384, E)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        # Using MultiheadAttention with batch_first=True for efficiency\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connections are vital for deep Transformers\n",
    "        y, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        x = x + y\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, patch_h=8, patch_w=16, embed_dim=768, depth=12, heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_h=patch_h, patch_w=patch_w, in_ch=3, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # [CLS] token for global classification\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # üî¥ FIX: Position embedding must match 384 patches + 1 CLS token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim=embed_dim, heads=heads, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Concatenate CLS token to the start of the sequence\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add learned spatial/positional encodings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # Use only the CLS token feature for the final prediction\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "# Instantiate with corrected patch sizes\n",
    "model = ViTClassifier(patch_h=8, patch_w=16).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"ViT trainable parameters: {num_params:,}\")\n",
    "\n",
    "# Forward sanity check using the batch from Cell 3\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x_batch.to(device))\n",
    "\n",
    "print(\"‚úì Forward OK | logits shape:\", logits.shape)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"‚úì Peak GPU memory used (GB): {peak_mem:.2f}\")\n",
    "\n",
    "print(f\"‚è± Cell 4 time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6d907",
   "metadata": {},
   "source": [
    "#### Cell 4 ‚Äî What this does\n",
    "- Defines a ViT-B/16-like classifier from scratch.\n",
    "- Confirms that the model forward pass works on a real batch.\n",
    "- Prints peak GPU memory usage for sanity.\n",
    "\n",
    "#### Future improvements\n",
    "- Try smaller models for ablation (e.g., depth=8, embed_dim=512).\n",
    "- Add regularisation tuning (dropout, stochastic depth) for full-scale training.\n",
    "- Consider pretraining (foundation model) before supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362d62cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "AMP enabled: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9514815956834d56a4a48645d0340ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Train]:   0%|          | 0/3162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m     logits = model(imgs)\n\u001b[32m     88\u001b[39m     loss = criterion(logits, labels)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m scaler.step(optimizer)\n\u001b[32m     92\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\IIT\\L6\\FYP\\ChagaSight\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\IIT\\L6\\FYP\\ChagaSight\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\IIT\\L6\\FYP\\ChagaSight\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5 ‚Äî Training loop FIXED (AMP warnings removed + avg loss + full logging)\n",
    "# =========================\n",
    "import time, json\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# -------------------------\n",
    "# Training configuration\n",
    "# -------------------------\n",
    "num_epochs = 20\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.05\n",
    "\n",
    "# BCEWithLogitsLoss is research-standard for soft labels (0.2/0.8)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "# NEW AMP API (removes FutureWarnings for PyTorch 2.0+)\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "best_val_auc = 0.0\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"AMP enabled:\", use_amp)\n",
    "\n",
    "# -------------------------\n",
    "# Corrected Config Logging\n",
    "# -------------------------\n",
    "config = {\n",
    "    \"experiment_name\": EXP_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"seed\": SEED,\n",
    "    \"datasets\": [\"ptbxl\", \"sami_trop\", \"code15\"],\n",
    "    \"subset_frac\": subset_frac,\n",
    "    \"train/val/test_sizes\": {\"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df)},\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"amp\": use_amp,\n",
    "    \"input_shape\": [3, 24, 2048],\n",
    "    # üî¥ FIX: Reflect the rectangular patch size (8x16) from Cell 4\n",
    "    \"vit\": {\"patch_h\": 8, \"patch_w\": 16, \"embed_dim\": 768, \"depth\": 12, \"heads\": 12, \"mlp_ratio\": 4.0, \"dropout\": 0.1},\n",
    "}\n",
    "if device.type == \"cuda\":\n",
    "    config[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "    config[\"vram_gb\"] = float(torch.cuda.get_device_properties(0).total_memory / 1e9)\n",
    "\n",
    "with open(EXP_DIR / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# Epoch loop\n",
    "# -------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "\n",
    "    for imgs, labels in train_bar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Automatic Mixed Precision for faster training on RTX 3050\n",
    "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    train_loss = running_loss / max(1, n_batches)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval()\n",
    "    val_preds, val_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            # Sigmoid is required to convert logits to probabilities for AUROC\n",
    "            probs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
    "            val_preds.extend(probs)\n",
    "            val_trues.extend(labels.numpy())\n",
    "\n",
    "    val_trues = np.asarray(val_trues)\n",
    "    val_preds = np.asarray(val_preds)\n",
    "\n",
    "    # Metric Evaluation: Binarize labels at 0.5 threshold\n",
    "    val_trues_bin = (val_trues > 0.5).astype(int)\n",
    "    val_auc = roc_auc_score(val_trues_bin, val_preds)\n",
    "\n",
    "    # Save best model based on AUROC\n",
    "    improved = \"\"\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        improved = \"‚úÖ\"\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_avg\": float(train_loss),\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"epoch_time_sec\": float(epoch_time),\n",
    "        \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"loss(avg)={train_loss:.4f} | \"\n",
    "        f\"val AUROC={val_auc:.4f} {improved} | \"\n",
    "        f\"time={epoch_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "# Save metrics for research reporting\n",
    "pd.DataFrame(history).to_csv(EXP_DIR / \"metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"Best validation AUROC:\", best_val_auc)\n",
    "print(\"Saved best model to:\", best_model_path)\n",
    "print(\"Saved metrics to:\", EXP_DIR / \"metrics.csv\")\n",
    "print(\"Saved config to:\", EXP_DIR / \"config.json\")\n",
    "print(f\"‚è± Cell 5 total time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e2eee",
   "metadata": {},
   "source": [
    "#### Cell 5 ‚Äî What this does\n",
    "- Trains the ViT model for a fixed number of epochs.\n",
    "- Uses AMP (mixed precision) to reduce VRAM usage and improve speed on the RTX 3050.\n",
    "- Computes validation AUROC each epoch (binary threshold only for the metric, not for training).\n",
    "- Saves a fully reproducible experiment record:\n",
    "  - `config.json` (model + hyperparameters + data sizes + GPU info)\n",
    "  - `metrics.csv` (epoch-by-epoch loss, AUROC, timing, LR)\n",
    "  - `model_best.pth` (best checkpoint selected by validation AUROC)\n",
    "\n",
    "#### Future improvements\n",
    "- Increase epochs for full-scale training (10‚Äì50).\n",
    "- Add early stopping based on AUROC plateau.\n",
    "- Replace oversampling with class-weighted loss or focal loss (ablation).\n",
    "- Add AUPRC and threshold metrics for clinical relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 6 ‚Äî Test evaluation + save test_results.json (FINAL)\n",
    "# =========================\n",
    "import time, json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Load Best Weights\n",
    "# -------------------------\n",
    "best_model_path = EXP_DIR / \"model_best.pth\"\n",
    "if not best_model_path.exists():\n",
    "    raise FileNotFoundError(f\"Best model checkpoint not found at {best_model_path}!\")\n",
    "\n",
    "# Use weights_only=True for security if using PyTorch 2.4+\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best research model from: {best_model_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Inference Loop\n",
    "# -------------------------\n",
    "test_preds = []\n",
    "test_trues = []\n",
    "test_datasets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Note: test_loader uses the natural distribution of data\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Inference on Test Set\"):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        \n",
    "        # Logits to probabilities via sigmoid\n",
    "        probs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
    "        \n",
    "        test_preds.extend(probs)\n",
    "        test_trues.extend(labels.numpy())\n",
    "\n",
    "test_preds = np.asarray(test_preds)\n",
    "test_trues = np.asarray(test_trues)\n",
    "test_trues_bin = (test_trues > 0.5).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# Global Metrics\n",
    "# -------------------------\n",
    "test_auc = roc_auc_score(test_trues_bin, test_preds)\n",
    "test_ap = average_precision_score(test_trues_bin, test_preds) # Added AUPRC for research depth\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"       FINAL TEST RESULTS\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Global AUROC : {test_auc:.4f}\")\n",
    "print(f\"Global AUPRC : {test_ap:.4f}\")\n",
    "print(f\"Test Samples : {len(test_trues)}\")\n",
    "print(f\"‚è± Evaluation time: {test_time:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Research Extension: Subgroup Analysis\n",
    "# -------------------------\n",
    "# This is a critical talking point for your viva!\n",
    "# It shows how the model generalizes across different data sources.\n",
    "subgroup_results = {}\n",
    "test_df_reset = test_df.reset_index(drop=True)\n",
    "\n",
    "for ds_name in test_df_reset[\"dataset\"].unique():\n",
    "    mask = test_df_reset[\"dataset\"] == ds_name\n",
    "    if mask.sum() > 0:\n",
    "        ds_preds = test_preds[mask]\n",
    "        ds_trues_bin = test_trues_bin[mask]\n",
    "        \n",
    "        # Check if subgroup has both classes to calculate AUC\n",
    "        if len(np.unique(ds_trues_bin)) > 1:\n",
    "            ds_auc = roc_auc_score(ds_trues_bin, ds_preds)\n",
    "            subgroup_results[ds_name] = float(ds_auc)\n",
    "            print(f\"Subgroup {ds_name:10} | AUROC: {ds_auc:.4f}\")\n",
    "        else:\n",
    "            print(f\"Subgroup {ds_name:10} | Only one class present (skipping AUC)\")\n",
    "\n",
    "# -------------------------\n",
    "# Persistence\n",
    "# -------------------------\n",
    "test_results = {\n",
    "    \"global_metrics\": {\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"test_auprc\": float(test_ap),\n",
    "        \"num_test_samples\": int(len(test_trues)),\n",
    "        \"evaluation_time_sec\": float(test_time)\n",
    "    },\n",
    "    \"subgroup_auroc\": subgroup_results,\n",
    "    \"model_architecture\": \"ViT-Rectangular-Patch-8x16\",\n",
    "    \"normalization\": \"Zero-Centered-127.5\"\n",
    "}\n",
    "\n",
    "with open(EXP_DIR / \"test_results.json\", \"w\") as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Saved comprehensive results to:\", EXP_DIR / \"test_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a72223",
   "metadata": {},
   "source": [
    "##### Cell 6 ‚Äî What this does\n",
    "- Loads the best checkpoint from this run folder.\n",
    "- Evaluates on the held-out test split.\n",
    "- Saves `test_results.json` so results are permanently stored.\n",
    "\n",
    "##### Future improvements\n",
    "- Report confidence intervals via bootstrapping (important for medical AI).\n",
    "- Add subgroup evaluation by dataset (PTB-XL vs SaMi-Trop vs CODE-15).\n",
    "- Add threshold-based metrics (sensitivity, specificity) for clinical interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 7 ‚Äî Visual Debugger: RA/LA/LL Contours & Model Inference\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def run_inference(img_tensor, label=None, record_info=\"Manual Test\"):\n",
    "    model.eval()\n",
    "    img_batch = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # 1. Run Inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_batch)\n",
    "        probability = torch.sigmoid(logits).item()\n",
    "    \n",
    "    # 2. Visualization of the 3 RA/LA/LL Channels\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\n",
    "    channel_names = [\n",
    "        \"Channel 1: RA-Reference (Projections: LL, V1-V6, LA)\",\n",
    "        \"Channel 2: LA-Reference (Projections: RA, V1-V6, LL)\",\n",
    "        \"Channel 3: LL-Reference (Projections: RA, V1-V6, LA)\"\n",
    "    ]\n",
    "    \n",
    "    # Convert back from normalized [-1, 1] to [0, 255] for visual clarity\n",
    "    display_img = ((img_tensor.cpu().numpy() + 1.0) * 127.5).astype(np.uint8)\n",
    "    \n",
    "    for i in range(3):\n",
    "        # We display each 8-row lead-contour stack\n",
    "        img_plot = axes[i].imshow(display_img[i], aspect='auto', cmap='magma')\n",
    "        axes[i].set_title(channel_names[i], fontsize=10)\n",
    "        axes[i].set_ylabel(\"Leads (8)\")\n",
    "        plt.colorbar(img_plot, ax=axes[i], fraction=0.01, pad=0.01)\n",
    "    \n",
    "    plt.xlabel(\"Temporal Axis (2048 samples - 10 Seconds)\")\n",
    "    plt.suptitle(f\"Spatial Contour Visualizer - {record_info}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Final Report\n",
    "    print(f\"\\n\" + \"=\"*30)\n",
    "    print(f\"   INFERENCE REPORT: {record_info}\")\n",
    "    print(\"=\"*30)\n",
    "    if label is not None:\n",
    "        print(f\"Ground Truth    : {'POSITIVE' if label > 0.5 else 'NEGATIVE'} ({label:.2f})\")\n",
    "    print(f\"Model Prediction : {'POSITIVE' if probability > 0.5 else 'NEGATIVE'}\")\n",
    "    print(f\"Confidence Level : {probability*100:.2f}%\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- CHOOSE YOUR MODE ---\n",
    "MANUAL_PATH = \"\" # e.g., \"data/processed/2d_images/code15/1000010_img.npy\"\n",
    "\n",
    "if MANUAL_PATH:\n",
    "    # Manual mode: Enter any relative path to an .npy file\n",
    "    full_path = PROJECT_ROOT / Path(MANUAL_PATH.replace(\"\\\\\", \"/\"))\n",
    "    if full_path.exists():\n",
    "        raw_img = np.load(full_path).astype(np.float32)\n",
    "        # Apply the exact normalization used in Cell 3\n",
    "        norm_img = torch.from_numpy((raw_img / 127.5) - 1.0)\n",
    "        run_inference(norm_img, record_info=f\"File: {full_path.name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: File not found at {full_path}\")\n",
    "else:\n",
    "    # Random Test mode: Pick a record from your loaded test dataset\n",
    "    idx = np.random.randint(0, len(test_ds))\n",
    "    img_t, lbl = test_ds[idx]\n",
    "    run_inference(img_t, label=lbl, record_info=f\"Test Dataset Index {idx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
