{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612a80f4",
   "metadata": {},
   "source": [
    "# ChagaSight — Vision Transformer (Baseline Training)\n",
    "\n",
    "Baseline ViT training on 2D ECG contour images  \n",
    "Datasets: PTB-XL (negatives), SaMi-Trop (positives), CODE-15 (soft labels)\n",
    "\n",
    "Baseline configuration:\n",
    "- 1% subset (pipeline verification)\n",
    "- No data augmentation\n",
    "- AMP enabled\n",
    "- Strict data integrity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f368854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "VRAM (GB): 6.441926656\n",
      "PROJECT_ROOT: d:\\IIT\\L6\\FYP\\ChagaSight\n",
      "DATA_DIR: d:\\IIT\\L6\\FYP\\ChagaSight\\data\\processed\n",
      "⏱ Cell 1 time: 0.00s\n"
     ]
    }
   ],
   "source": [
    "import time, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", torch.cuda.get_device_properties(0).total_memory / 1e9)\n",
    "\n",
    "# Find project root robustly (VS Code safe)\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "print(f\"⏱ Cell 1 time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e34c010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total metadata rows: 63228\n",
      "Rows after integrity filter: 63228\n",
      "Subset records (1%): 632\n",
      "Train: 505 | Val: 63 | Test: 64\n",
      "⏱ Cell 2 time: 3.33s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2 — Metadata loading + integrity filtering + 1% subset\n",
    "# =========================\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "datasets = [\"ptbxl\", \"sami_trop\", \"code15\"]\n",
    "dfs = []\n",
    "\n",
    "# Load metadata CSVs\n",
    "for ds in datasets:\n",
    "    csv_path = DATA_DIR / \"metadata\" / f\"{ds}_metadata.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing metadata CSV: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"dataset\"] = ds\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Total metadata rows:\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# HARD integrity filter (RELATIVE PATH SAFE)\n",
    "# -------------------------\n",
    "def img_exists(p):\n",
    "    return (PROJECT_ROOT / Path(p)).exists()\n",
    "\n",
    "exists_mask = df_all[\"img_path\"].apply(img_exists)\n",
    "missing_count = (~exists_mask).sum()\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"⚠️ Dropping {missing_count} rows with missing image files\")\n",
    "    print(df_all.loc[~exists_mask, [\"dataset\", \"img_path\"]].head())\n",
    "\n",
    "df_all = df_all.loc[exists_mask].reset_index(drop=True)\n",
    "print(\"Rows after integrity filter:\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# START SMALL: 1% subset\n",
    "# -------------------------\n",
    "subset_frac = 0.01\n",
    "df_all = df_all.sample(frac=subset_frac, random_state=SEED).reset_index(drop=True)\n",
    "print(\"Subset records (1%):\", len(df_all))\n",
    "\n",
    "# -------------------------\n",
    "# Binary label ONLY for stratification / metrics\n",
    "# -------------------------\n",
    "df_all[\"label_bin\"] = (df_all[\"label\"] > 0.5).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Val / Test split\n",
    "# -------------------------\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.2,\n",
    "    stratify=df_all[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label_bin\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"⏱ Cell 2 time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b86757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DataLoader sanity check...\n",
      "✓ Batch image shape : torch.Size([16, 3, 24, 2048])\n",
      "✓ Batch label shape : torch.Size([16])\n",
      "✓ Sample labels    : [0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.0, 0.20000000298023224, 1.0, 0.800000011920929, 0.20000000298023224, 0.800000011920929, 1.0]\n",
      "✓ Image value range : [0.000, 255.000]\n",
      "⏱ Cell 3 time: 0.03s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 — Dataset + DataLoaders (baseline, no augmentation)\n",
    "# =========================\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class ECGImageDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Resolve relative path safely\n",
    "        img_path = PROJECT_ROOT / Path(row[\"img_path\"])\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image file: {img_path}\")\n",
    "\n",
    "        img = np.load(img_path).astype(np.float32)\n",
    "\n",
    "        # Strict shape check (research safety)\n",
    "        if img.shape != (3, 24, 2048):\n",
    "            raise ValueError(f\"Invalid image shape {img.shape} at {img_path}\")\n",
    "\n",
    "        img = torch.from_numpy(img)  # (3, 24, 2048)\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DataLoaders\n",
    "# -------------------------\n",
    "batch_size = 16  # RTX 3050 (6GB) safe\n",
    "\n",
    "train_ds = ECGImageDataset(train_df)\n",
    "val_ds   = ECGImageDataset(val_df)\n",
    "test_ds  = ECGImageDataset(test_df)\n",
    "\n",
    "# Oversample confident positives (CODE-15 & SaMi-Trop)\n",
    "weights = train_df[\"label\"].apply(lambda x: 10.0 if x > 0.7 else 1.0).values\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,      # Windows-safe\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Sanity check: first batch\n",
    "# -------------------------\n",
    "print(\"Running DataLoader sanity check...\")\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"✓ Batch image shape :\", x_batch.shape)   # (16, 3, 24, 2048)\n",
    "print(\"✓ Batch label shape :\", y_batch.shape)\n",
    "print(\"✓ Sample labels    :\", y_batch[:10].tolist())\n",
    "print(\n",
    "    f\"✓ Image value range : \"\n",
    "    f\"[{x_batch.min().item():.3f}, {x_batch.max().item():.3f}]\"\n",
    ")\n",
    "\n",
    "print(f\"⏱ Cell 3 time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d52c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT trainable parameters: 85,747,201\n",
      "Model device: cuda:0\n",
      "✓ Forward pass OK\n",
      "✓ Logits shape : torch.Size([16])\n",
      "✓ GPU memory used (GB): 2.46\n",
      "⏱ Cell 4 time: 0.59s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4 — Vision Transformer model + forward sanity test\n",
    "# =========================\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Patch Embedding\n",
    "# -------------------------\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_ch=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_ch,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        self.num_patches = (24 // patch_size) * (2048 // patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                  # (B, E, H', W')\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, N, E)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Transformer Block\n",
    "# -------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        x = x + y\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ViT Classifier\n",
    "# -------------------------\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=16,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=patch_size,\n",
    "            in_ch=3,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                heads=heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, 1)  # binary logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Instantiate model\n",
    "# -------------------------\n",
    "model = ViTClassifier().to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"ViT trainable parameters: {num_params:,}\")\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "# -------------------------\n",
    "# Forward sanity test\n",
    "# -------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_batch_gpu = x_batch.to(device)\n",
    "    logits = model(x_batch_gpu)\n",
    "\n",
    "print(\"✓ Forward pass OK\")\n",
    "print(\"✓ Logits shape :\", logits.shape)  # (B,)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"✓ GPU memory used (GB): {mem:.2f}\")\n",
    "\n",
    "print(f\"⏱ Cell 4 time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72de5977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "AMP enabled: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanuth Mandepa\\AppData\\Local\\Temp\\ipykernel_18984\\3933605267.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc3ad2ca28845e8a1c633e9a03c215d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [Train]:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanuth Mandepa\\AppData\\Local\\Temp\\ipykernel_18984\\3933605267.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac4e0aa6ca444eae8729d8e7dd67ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [Val]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=1.0297 | val AUROC=0.8033 ✅ | time=10.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5febd99e8c407795a21d267ec95770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [Train]:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanuth Mandepa\\AppData\\Local\\Temp\\ipykernel_18984\\3933605267.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f304cc56a07488298296e52a242613d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [Val]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.6076 | val AUROC=0.9098 ✅ | time=10.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13bd15d2a4b4050abd5151b2f92d2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [Train]:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanuth Mandepa\\AppData\\Local\\Temp\\ipykernel_18984\\3933605267.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058179204e694edda2a1da374b58970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [Val]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss=0.6258 | val AUROC=0.9672 ✅ | time=9.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c6bc13ab7440ce967ca3a05389a079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [Train]:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanuth Mandepa\\AppData\\Local\\Temp\\ipykernel_18984\\3933605267.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66b9a49add146b6a419fc5f6c64cdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [Val]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=0.6047 | val AUROC=0.9672  | time=9.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421e4081ace543d5ae503e54c9429c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [Train]:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanuth Mandepa\\AppData\\Local\\Temp\\ipykernel_18984\\3933605267.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64753e1bf6df4529a26cf5c6e5ad9beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [Val]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | loss=0.6063 | val AUROC=0.9672  | time=9.7s\n",
      "\n",
      "Training complete.\n",
      "Best validation AUROC: 0.9672131147540983\n",
      "⏱ Cell 5 total time: 49.57s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5 — Training loop (baseline, AMP, progress, timing)\n",
    "# =========================\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Training configuration\n",
    "# -------------------------\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.05\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs\n",
    ")\n",
    "\n",
    "use_amp = device.type == \"cuda\"\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "best_val_auc = 0.0\n",
    "best_model_path = MODEL_DIR / \"vit_baseline_best.pth\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"AMP enabled:\", use_amp)\n",
    "\n",
    "# -------------------------\n",
    "# Epoch loop\n",
    "# -------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\",\n",
    "        leave=False\n",
    "    )\n",
    "\n",
    "    for imgs, labels in train_bar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(\n",
    "            val_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\",\n",
    "            leave=False\n",
    "        ):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            probs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
    "\n",
    "            val_preds.extend(probs)\n",
    "            val_trues.extend(labels.numpy())\n",
    "\n",
    "    val_trues = np.asarray(val_trues)\n",
    "    val_preds = np.asarray(val_preds)\n",
    "\n",
    "    # Binarise labels for metrics ONLY\n",
    "    val_trues_bin = (val_trues > 0.5).astype(int)\n",
    "\n",
    "    val_auc = roc_auc_score(val_trues_bin, val_preds)\n",
    "\n",
    "    # Save best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        improved = \"✅\"\n",
    "    else:\n",
    "        improved = \"\"\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"loss={train_loss:.4f} | \"\n",
    "        f\"val AUROC={val_auc:.4f} {improved} | \"\n",
    "        f\"time={time.time() - epoch_start:.1f}s\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"Best validation AUROC:\", best_val_auc)\n",
    "print(f\"⏱ Cell 5 total time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "089b69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: d:\\IIT\\L6\\FYP\\ChagaSight\\models\\vit_baseline_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ecdaa3d8ef48de8ca7030cfaa97404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test evaluation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST RESULTS ===\n",
      "Test AUROC : 0.9194\n",
      "⏱ Cell 6 time: 1.09s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6 — Test evaluation (held-out set)\n",
    "# =========================\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Load best model\n",
    "# -------------------------\n",
    "best_model_path = MODEL_DIR / \"vit_baseline_best.pth\"\n",
    "assert best_model_path.exists(), \"Best model checkpoint not found!\"\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded best model from:\", best_model_path)\n",
    "\n",
    "# -------------------------\n",
    "# Test loop\n",
    "# -------------------------\n",
    "test_preds = []\n",
    "test_trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Test evaluation\"):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "\n",
    "        probs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
    "        test_preds.extend(probs)\n",
    "        test_trues.extend(labels.numpy())\n",
    "\n",
    "test_preds = np.asarray(test_preds)\n",
    "test_trues = np.asarray(test_trues)\n",
    "\n",
    "# -------------------------\n",
    "# Metrics (binary labels ONLY for metrics)\n",
    "# -------------------------\n",
    "test_trues_bin = (test_trues > 0.5).astype(int)\n",
    "test_auc = roc_auc_score(test_trues_bin, test_preds)\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "print(f\"Test AUROC : {test_auc:.4f}\")\n",
    "print(f\"⏱ Cell 6 time: {time.time() - start_time:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
