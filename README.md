ğŸ§¬ **ChagaSight â€” A Vision Transformerâ€“Based ECG Image Pipeline for Chagas Disease Detection**

A Final-Year Research Project using Physiologically Structured 2D ECG Images and Optional 1D ECG Foundation Models

ChagaSight is a modular deep-learning framework designed to detect Chagas disease from 12-lead ECGs.
This project focuses primarily on transforming ECG signals into physiologically structured 2D images and training a Vision Transformer (ViT) classifier on these images.

In addition, ChagaSight includes an optional extension exploring a 1D ECG Foundation Model (FM) based on masked self-supervised pretraining (ST-MEM), and an optional hybrid alignment model that links 1D signal embeddings with 2D image embeddings.

The approach is inspired by two modern research pipelines:

Physiologically Structured 2D ECG Image Embedding (2025)

Vision Transformer Foundation Model for ECGs (2025)

# ğŸ“Œ 1. Project Overview

ChagaSight provides an end-to-end workflow for multi-dataset ECG processing, image generation, model training, and evaluation.

âœ” Multi-Dataset ECG Preprocessing
PTB-XL, CODE-15%, and SaMi-Trop are unified by cleaning, resampling, trimming, and normalizing all signals.

âœ” 2D ECG Image Representation (PRIMARY METHOD)
ECGs are converted into structured 3-channel images using RA/LA/LL contour mapping, producing ViT-ready images (3 Ã— 24 Ã— 2048).

âœ” Vision Transformer (ViT) Image Classifier (MAIN MODEL)
The primary dissertation model: a ViT trained on 2D ECG images to classify Chagas disease.

âœ” Optional: ECG Foundation Model (1D ViT-FM)
A transformer encoder trained using ST-MEM masked reconstruction for advanced representation learning.

âœ” Optional: Hybrid FM + ViT Alignment
A REPA-style cosine alignment loss linking 1D FM embeddings with 2D ViT image embeddings for robustness.
This structure enables a scalable, research-grade pipeline suitable for academic evaluation and future clinical studies.

---

# ğŸ“Š 2. Supported Datasets

ChagaSight supports three widely used 12-lead ECG datasets in modern ECG AI research:

| Dataset       | Description                          | Sample Rate | Chagas Label               |
| ------------- | ------------------------------------ | ----------- | -------------------------- |
| **PTB-XL**    | 21k European ECGs                    | 100/500 Hz  | All **negative (0)**       |
| **CODE-15%**  | 345k Brazilian ECGs                  | 400 Hz      | **Soft labels: 0.2 / 0.8** |
| **SaMi-Trop** | 1631 serology-confirmed Chagas cases | 400 Hz      | All **positive (1)**       |

### âœ” Label Policy

Consistent with state-of-the-art research:

| Dataset   | Confidence           | Assigned Label |
| --------- | -------------------- | -------------- |
| PTB-XL    | Very strong negative | **0**          |
| SaMi-Trop | Very strong positive | **1**          |
| CODE-15%  | Weak (self-reported) | **0.2 / 0.8**  |

---

# ğŸ“ 3. Folder Structure (2025 Architecture)

ChagaSight/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original unmodified ECG datasets
â”‚ â”‚ â”œâ”€â”€ ptbxl/ # PTB-XL (100 Hz / 500 Hz WFDB files)
â”‚ â”‚ â”œâ”€â”€ code15/ # CODE-15% Brazil dataset (raw ECGs)
â”‚ â”‚ â””â”€â”€ sami_trop/ # SaMi-Trop serology-confirmed Chagas dataset
â”‚ â”‚
â”‚ â”œâ”€â”€ processed/
â”‚ â”‚ â”œâ”€â”€ 1d_signals/ # Cleaned, resampled ECG (1D numpy arrays)
â”‚ â”‚ â”‚ # â†’ Baseline-removed, resampled, normalized
â”‚ â”‚ â””â”€â”€ 2d_images/ # 2D structured ECG images (3 Ã— 24 Ã— 2048)
â”‚ â”‚ # â†’ Final input format for Vision Transformer
â”‚ â”‚
â”‚ â””â”€â”€ splits/ # Patient-level train/val/test splits (JSON)
â”‚
â”œâ”€â”€ src/ # All source code
â”‚ â”œâ”€â”€ preprocessing/
â”‚ â”‚ â”œâ”€â”€ baseline_removal.py # High-pass / band-pass filtering
â”‚ â”‚ â”œâ”€â”€ resample.py # Resampling to 400 Hz + padding & trimming
â”‚ â”‚ â”œâ”€â”€ normalization.py # Per-lead z-score normalization utilities
â”‚ â”‚ â”œâ”€â”€ image_embedding.py # ECG â†’ 2D image conversion (RA/LA/LL channels)
â”‚ â”‚ â””â”€â”€ soft_labels.py # Soft-label generation for CODE-15% dataset
â”‚ â”‚
â”‚ â”œâ”€â”€ foundation_model/ # OPTIONAL â€” For ECG Foundation Model (1D FM)
â”‚ â”‚ â”œâ”€â”€ vit_1d_encoder.py # 1D ViT backbone for ECG signals
â”‚ â”‚ â”œâ”€â”€ st_mem_pretraining.py # Masked self-supervised training (ST-MEM)
â”‚ â”‚ â”œâ”€â”€ aol_mixing.py # Aggregation of Layers (AoL) module
â”‚ â”‚ â””â”€â”€ fm_feature_extractor.py # Extract FM embeddings for hybrid models
â”‚ â”‚
â”‚ â”œâ”€â”€ image_model/ # MAIN MODEL â€” ViT image classifier
â”‚ â”‚ â”œâ”€â”€ vit_image_encoder.py # Vision Transformer backbone for ECG images
â”‚ â”‚ â”œâ”€â”€ projection_head.py # Linear head for classification
â”‚ â”‚ â””â”€â”€ alignment_loss.py # Loss for hybrid FM + ViT alignment
â”‚ â”‚
â”‚ â”œâ”€â”€ dataloaders/
â”‚ â”‚ â”œâ”€â”€ ptbxl_loader.py # PTB-XL dataloader (1D + 2D modes)
â”‚ â”‚ â”œâ”€â”€ code15_loader.py # CODE-15% loader with soft labels
â”‚ â”‚ â”œâ”€â”€ sami_loader.py # SaMi-Trop Chagas dataset loader
â”‚ â”‚ â”œâ”€â”€ fm_signal_dataset.py # Dataset for training 1D Foundation Model (FM)
â”‚ â”‚ â””â”€â”€ image_dataset.py # Dataloader for ECG image-based ViT training
â”‚ â”‚
â”‚ â”œâ”€â”€ training/
â”‚ â”‚ â”œâ”€â”€ train_image_model.py # MAIN TRAINER â€” Vision Transformer training
â”‚ â”‚ â”œâ”€â”€ train_fm.py # OPTIONAL â€” Training the ECG Foundation Model
â”‚ â”‚ â”œâ”€â”€ train_hybrid.py # OPTIONAL â€” FM + ViT hybrid alignment training
â”‚ â”‚ â”œâ”€â”€ augmentations_1d.py # 1D ECG augmentations for FM
â”‚ â”‚ â””â”€â”€ augmentations_2d.py # 2D ECG image augmentations for ViT
â”‚ â”‚
â”‚ â””â”€â”€ evaluation/
â”‚ â”œâ”€â”€ metrics.py # AUROC, AUPRC, F1, calibration metrics
â”‚ â”œâ”€â”€ explainability.py # Grad-CAM for images + FM attention maps
â”‚ â””â”€â”€ challenge_metric.py # Top-K TPR scoring utilities
â”‚
â”œâ”€â”€ scripts/ # Executable scripts for full pipeline
â”‚ â”œâ”€â”€ preprocess_ptbxl.py # Preprocess PTB-XL (Stage 1: 1D)
â”‚ â”œâ”€â”€ preprocess_code15.py # Preprocess CODE-15% (Stage 1)
â”‚ â”œâ”€â”€ preprocess_samitrop.py # Preprocess SaMi-Trop (Stage 1)
â”‚ â”œâ”€â”€ build_images.py # Stage 2: ECG â†’ 2D image creation
â”‚ â”œâ”€â”€ create_splits.py # Build train/val/test splits
â”‚ â”œâ”€â”€ train_vit_image.sh # Shell script: Train ViT model
â”‚ â”œâ”€â”€ train_fm.sh # Shell script: Train FM model (optional)
â”‚ â””â”€â”€ train_hybrid.sh # Shell script: Hybrid alignment training
â”‚
â”œâ”€â”€ notebooks/ # Development + documentation notebooks
â”‚ â”œâ”€â”€ 01_preprocessing_1d.ipynb # Preprocess ECG into 1D format
â”‚ â”œâ”€â”€ 02_image_embedding.ipynb # Convert 1D â†’ 2D images
â”‚ â”œâ”€â”€ 03_fm_pretraining.ipynb # OPTIONAL â€” FM masked training
â”‚ â”œâ”€â”€ 04_cross_validation.ipynb # Model validation experiments
â”‚ â”œâ”€â”€ 05_hybrid_alignment_training.ipynb
â”‚ â””â”€â”€ 06_evaluation.ipynb # Visualisations + performance metrics
â”‚
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ methodology/ # Dissertation-ready documentation
â”‚ â”‚ â”œâ”€â”€ fm_architecture.md # 1D FM architecture explanation
â”‚ â”‚ â”œâ”€â”€ image_embedding_diagram.png # 2D ECG image pipeline visualisation
â”‚ â”‚ â””â”€â”€ augmentation_strategy.md # Full augmentation design
â”‚ â”‚
â”‚ â”œâ”€â”€ figures/ # Figures for thesis/report
â”‚ â”œâ”€â”€ reports/ # Auto-generated experiment summaries
â”‚ â””â”€â”€ diagrams/ # Flowcharts, system diagrams, etc.
â”‚
â”œâ”€â”€ experiments/ # Saved experimental runs
â”‚ â”œâ”€â”€ image_baseline/ # ViT image model results
â”‚ â”œâ”€â”€ fm_pretraining/ # FM pretraining logs
â”‚ â””â”€â”€ hybrid_alignment/ # Hybrid model experiments
â”‚
â”œâ”€â”€ results/ # Outputs (excluded from Git)
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation

---

# ğŸ”§ 4. Preprocessing Pipeline

ChagaSight adopts a **two-stage preprocessing strategy** inspired by recent ECG research.

## **Stage 1 â€” 1D Signal Preprocessing**

- Baseline drift removal
- Resampling to a unified frequency
- Padding/trimming to fixed duration (10s)
- Per-lead z-score normalization
- Saving as .npy 1D arrays

Output directory:
data/processed/1d_signals/

## **Stage 2 â€” ECG â†’ Structured 2D Image Conversion**

Using physiologically meaningful RA/LA/LL contour mapping:

- Construct 3 channels representing RA, LA, LL contours
- Subtract reference lead (augmented limb lead)
- Clip amplitudes to [-3, 3]
- Scale to pixel range [0â€“255]
- Resize to 3 Ã— 24 Ã— 2048

Output directory:
data/processed/2d_images/

This format is optimized for Vision Transformers.

---

# ğŸ§  5. Model Components

## **A. Vision Transformer (MAIN MODEL)**

- Input: structured ECG images
- Patch embeddings adapted for rectangular biomedical images
- Output: disease probability + latent embeddings

## This is the primary deliverable of the final-year project.

## **B. ECG Foundation Model (OPTIONAL EXTENSION)**

A 1D Vision Transformer (12 layers) trained using:

- ST-MEM masked self-supervised learning
- Patch size = 50
- Aggregation of Layers (AoL)

Provides robust signal-level ECG embeddings, especially useful in low-label or multi-dataset setups.

---

## **C. Hybrid FM + ViT Alignment Model (Optional Advanced Model)**

A REPA-inspired loss encourages ViT and FM feature alignment:

L_total = L_classification + Î» Â· cosine_similarity(FM_features, ViT_features)

## Used for robustness and dataset-shift resistance.

# ğŸ§ª 6. Training Workflow

### **1. Preprocess 1D ECG Signals**

notebooks/01_preprocessing_1d.ipynb

### **2. Convert 1D Signals to 2D Images**

notebooks/02_image_embedding.ipynb

### **3. Train Vision Transformer (Main Model)**

python src/training/train_image_model.py

### **4. Evaluate ViT Model**

    AUROC
    AUPRC
    F1
    Calibration
    Grad-CAM

### **5. (Optional) Train ECG Foundation Model**

python src/training/train_fm.py

### **6. (Optional) Train Hybrid Model**

python src/training/train_hybrid.py

---

# ğŸ“ˆ 7. Evaluation Metrics

    AUROC
    AUPRC
    Accuracy & F1
    Top-K screening sensitivity
    Calibration curves
    Confusion matrices
    Grad-CAM (image & signal attention)

---

# ğŸ” 8. Key Contributions of This Pipeline

**âœ” Physiologically Structured 2D ECG Image Pipeline**
A high-fidelity image representation built on limb-lead reference mapping.

**âœ” Vision Transformer Baseline Model (Main Output)**
The primary focus of the dissertation.

**âœ” Optional 1D Foundation Model (Advanced)**
Implements contemporary masked ECG self-supervision.

**âœ” Optional Hybrid Feature Alignment**
Bridges image-based and signal-based features.

**âœ” Unified Multi-Dataset Workflow**
A single preprocessing and training pipeline across PTB-XL, CODE-15%, SaMi-Trop.

---

# ğŸš€ 9. Roadmap

[ ] Phase 1 â€” 1D ECG Preprocessing (Required)

    Clean and normalize all datasets
    Resample, trim, pad
    Save as .npy 1D signals

[ ] Phase 2 â€” Structured 2D Image Generation (Required)

    Produce 3-channel structured images
    Validate RA/LA/LL mapping
    Save to processed/2d_images/

[ ] Phase 3 â€” Train Vision Transformer (Required)

    Train ViT classifier
    Evaluate AUROC / AUPRC / F1

[ ] Phase 4 â€” ECG Foundation Model (Optional)

    ST-MEM masked pretraining
    Extract FM embeddings

[ ] Phase 5 â€” Hybrid Alignment (Optional)

    Train joint FM + ViT model
    Apply alignment loss

[ ] Phase 6 â€” Evaluation & Explainability (Required)

    Grad-CAM
    Calibration curves
    Dataset-shift analysis

[ ] Phase 7 â€” Dissertation Deliverables (Required)

    Write methodology chapter
    Include all diagrams
    Present results, comparison, limitations

---

# ğŸ“¬ Contact

See the `docs/` directory for methodology, architecture notes, and experiment logs.
